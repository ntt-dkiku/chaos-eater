{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a91303b6-b761-46e3-ab5a-1233e4b3284e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: nginx\n",
      "input_tokens {'openai/gpt-4o-2024-08-06': {'preprocess': [2579, 2583, 2593, 2601, 2578], 'hypothesis': [25560, 25217, 25667, 25345, 25304], 'experiment': [13341, 13039, 13263, 13321, 13397], 'analysis': [4579, 4383, 4232, 4449, 4411], 'improvement': [5611, 5466, 5279, 5454, 5499], 'summary': [8279, 7980, 8107, 8294, 8258], 'total': [59949, 58668, 59141, 59464, 59447]}}\n",
      "output_tokens {'openai/gpt-4o-2024-08-06': {'preprocess': [469, 457, 486, 488, 459], 'hypothesis': [2503, 2515, 2542, 2496, 2561], 'experiment': [1780, 1571, 1752, 1683, 1675], 'analysis': [580, 632, 601, 552, 636], 'improvement': [264, 249, 239, 234, 254], 'summary': [389, 331, 381, 313, 409], 'total': [5985, 5755, 6001, 5766, 5994]}}\n",
      "total_tokens {'openai/gpt-4o-2024-08-06': {'preprocess': [3048, 3040, 3079, 3089, 3037], 'hypothesis': [28063, 27732, 28209, 27841, 27865], 'experiment': [15121, 14610, 15015, 15004, 15072], 'analysis': [5159, 5015, 4833, 5001, 5047], 'improvement': [5875, 5715, 5518, 5688, 5753], 'summary': [8668, 8311, 8488, 8607, 8667], 'total': [65934, 64423, 65142, 65230, 65441]}}\n",
      "run_time {'openai/gpt-4o-2024-08-06': {'preprocess': [19.370300769805908, 22.39142656326294, 18.197205543518066, 24.666368007659912, 19.865697860717773], 'hypothesis': [150.01416754722595, 201.49848055839539, 165.05291843414307, 133.11122059822083, 144.65431022644043], 'experiment': [251.75681376457214, 332.3084635734558, 289.52038860321045, 216.11682987213135, 235.6310911178589], 'analysis': [37.468055725097656, 83.82978439331055, 60.87412762641907, 22.83656930923462, 44.05736517906189], 'improvement': [9.332541942596436, 15.290964365005493, 11.709984302520752, 13.02860164642334, 8.340997457504272], 'summary': [17.506391286849976, 9.74032473564148, 31.191861867904663, 17.825693130493164, 30.640270233154297], 'total': [586.0878698825836, 805.2311072349548, 704.1627848148346, 547.7209017276764, 583.0074605941772]}}\n",
      "completion {'openai/gpt-4o-2024-08-06': {'preprocess': [True, True, True, True, True], 'hypothesis': [True, True, True, True, True], 'experiment': [True, True, True, True, True], 'analysis': [True, True, True, True, True], 'improvement': [True, True, True, True, True], 'summary': [True, True, True, True, True], 'total': [True, True, True, True, True]}}\n",
      "review {'openai/gpt-4o-2024-08-06': {'preprocess': [], 'hypothesis': [], 'experiment': [], 'analysis': [], 'improvement': [], 'summary': [], 'total': []}}\n",
      "reconfig_rate {'openai/gpt-4o-2024-08-06': [True, True, True, True, True]}\n",
      "\n",
      "sample: sockshop\n",
      "input_tokens {'openai/gpt-4o-2024-08-06': {'preprocess': [30151, 30214, 30185, 30140, 30148], 'hypothesis': [141444, 172929, 135732, 154212, 143976], 'experiment': [57525, 57800, 56130, 56985, 57728], 'analysis': [14033, 14319, 14063, 14125], 'improvement': [15093, 15448, 15164, 15255], 'summary': [18879, 18953, 17845, 16525, 18687], 'total': [277125, 309663, 269119, 279919]}}\n",
      "output_tokens {'openai/gpt-4o-2024-08-06': {'preprocess': [5757, 5714, 5489, 5682, 5689], 'hypothesis': [3796, 4195, 3277, 3231, 4681], 'experiment': [1792, 1831, 1749, 1819, 1751], 'analysis': [605, 674, 651, 679], 'improvement': [737, 472, 746, 471], 'summary': [404, 494, 698, 465, 368], 'total': [13091, 13380, 12610, 13639]}}\n",
      "total_tokens {'openai/gpt-4o-2024-08-06': {'preprocess': [35908, 35928, 35674, 35822, 35837], 'hypothesis': [145240, 177124, 139009, 157443, 148657], 'experiment': [59317, 59631, 57879, 58804, 59479], 'analysis': [14638, 14993, 14714, 14804], 'improvement': [15830, 15920, 15910, 15726], 'summary': [19283, 19447, 18543, 16990, 19055], 'total': [290216, 323043, 281729, 293558]}}\n",
      "run_time {'openai/gpt-4o-2024-08-06': {'preprocess': [109.1295554637909, 325.5049293041229, 309.46681356430054, 311.2789568901062, 311.36255264282227], 'hypothesis': [185.09234619140625, 561.4890034198761, 130.56003189086914, 162.93013834953308, 240.24590587615967], 'experiment': [231.93876552581787, 245.7812523841858, 194.13719248771667, 125.4509003162384, 199.55498147010803], 'analysis': [25.803356885910034, 37.582271099090576, 32.37568736076355, 47.04048299789429], 'improvement': [265.6854648590088, 249.18899273872375, 263.8023569583893, 263.81188893318176], 'summary': [12.299491882324219, 24.15630340576172, 36.17111253738403, 18.605159044265747, 11.580004930496216], 'total': [1235.8817057609558, 1859.604516506195, 1369.1943287849426, 1483.1334664821625]}}\n",
      "completion {'openai/gpt-4o-2024-08-06': {'preprocess': [True, True, True, True, True], 'hypothesis': [True, True, True, True, True], 'experiment': [True, True, True, True, True], 'analysis': [True, True, True, True, True], 'improvement': [True, True, True, True, True], 'summary': [True, True, True, True, True], 'total': [True, True, True, True, True]}}\n",
      "review {'openai/gpt-4o-2024-08-06': {'preprocess': [], 'hypothesis': [], 'experiment': [], 'analysis': [], 'improvement': [], 'summary': [], 'total': []}}\n",
      "reconfig_rate {'openai/gpt-4o-2024-08-06': [True, True, True, False, True]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from typing import Tuple\n",
    "from chaos_eater.utils.functions import load_json\n",
    "from chaos_eater.chaos_eater import ChaosEaterOutput\n",
    "\n",
    "def get_tokens(ce_output: ChaosEaterOutput, phase_name: str) -> Tuple[int, int, int]:\n",
    "    logs = ce_output.logs[phase_name]\n",
    "    input_tokens = 0\n",
    "    output_tokens = 0\n",
    "    total_tokens = 0\n",
    "    for log in logs:\n",
    "        if isinstance(log, list):\n",
    "            for log_ in log:\n",
    "                token_usage = log_.token_usage\n",
    "                input_tokens += token_usage.input_tokens\n",
    "                output_tokens += token_usage.output_tokens\n",
    "                total_tokens += token_usage.total_tokens    \n",
    "        else:\n",
    "            token_usage = log.token_usage\n",
    "            input_tokens += token_usage.input_tokens\n",
    "            output_tokens += token_usage.output_tokens\n",
    "            total_tokens += token_usage.total_tokens\n",
    "    return input_tokens, output_tokens, total_tokens\n",
    "\n",
    "def get_runtime(ce_output: ChaosEaterOutput, phase_name: str) -> float:\n",
    "    run_time = ce_output.run_time[phase_name]\n",
    "    if isinstance(run_time, list):\n",
    "        total_run_time = 0.\n",
    "        for run_time_ in run_time:\n",
    "            total_run_time += run_time_\n",
    "        return total_run_time\n",
    "    else:\n",
    "        return ce_output.run_time[phase_name]\n",
    "    \n",
    "def get_dict_list():\n",
    "    return {\n",
    "        \"preprocess\": [],\n",
    "        \"hypothesis\": [],\n",
    "        \"experiment\": [],\n",
    "        \"analysis\": [],\n",
    "        \"improvement\": [],\n",
    "        \"summary\": [],\n",
    "        \"total\": []\n",
    "    }\n",
    "\n",
    "def extract_number(filename: str) -> int:\n",
    "    match = re.search(r'\\d+', filename)\n",
    "    if match:\n",
    "        return int(match.group(0))\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "RESULT_DIR = \"results\"\n",
    "MODELS = [\"openai/gpt-4o-2024-08-06\"] #, \"google/gemini-1.5-pro\", \"anthropic/claude-3-5-sonnet-20240620\"]\n",
    "SAMPLES = [\"nginx\", \"sockshop\"]\n",
    "\n",
    "\n",
    "for sample in SAMPLES:\n",
    "    input_tokens = {}\n",
    "    output_tokens = {}\n",
    "    total_tokens = {}\n",
    "    run_time = {}\n",
    "    completion = {}\n",
    "    score = {}\n",
    "    reconfig_rate = {}\n",
    "    for model in MODELS:\n",
    "        #----------------\n",
    "        # initialization\n",
    "        #----------------\n",
    "        input_tokens[model] = get_dict_list()\n",
    "        output_tokens[model] = get_dict_list()\n",
    "        total_tokens[model] = get_dict_list()\n",
    "        run_time[model] = get_dict_list()\n",
    "        completion[model] = get_dict_list()\n",
    "        score[model] = get_dict_list()\n",
    "        reconfig_rate[model] = []\n",
    "        \n",
    "        def add_metrics(phase_name: str):\n",
    "            if phase_name == \"experiment\":\n",
    "                input_tokens_, output_tokens_, total_tokens_ = get_tokens(ce_output, \"experiment_plan\")\n",
    "                run_time_ = get_runtime(ce_output, \"experiment_plan\") + get_runtime(ce_output, \"experiment_execution\")\n",
    "            else:\n",
    "                input_tokens_, output_tokens_, total_tokens_ = get_tokens(ce_output, phase_name)\n",
    "                run_time_ = get_runtime(ce_output, phase_name)\n",
    "            completion[model][phase_name].append(True)\n",
    "            input_tokens[model][phase_name].append(input_tokens_)\n",
    "            output_tokens[model][phase_name].append(output_tokens_)\n",
    "            total_tokens[model][phase_name].append(total_tokens_)\n",
    "            run_time[model][phase_name].append(run_time_)\n",
    "            return input_tokens_, output_tokens_, total_tokens_,\n",
    "    \n",
    "        #-------------\n",
    "        # aggregation\n",
    "        #-------------\n",
    "        result_paths = [f\"results/{model.split('/')[-1]}-{sample}-{i}/outputs/output.json\" for i in range(1, 6)]\n",
    "        # loop over samples\n",
    "        for path in result_paths:\n",
    "            ce_output = ChaosEaterOutput(**load_json(path))\n",
    "            #---------------\n",
    "            # preprocessing\n",
    "            #---------------\n",
    "            if ce_output.ce_cycle.processed_data is not None:\n",
    "                preprocess_input_tokens, preprocess_output_tokens, preprocess_total_tokens = add_metrics(\"preprocess\")\n",
    "            else:\n",
    "                completion[model][\"preprocess\"].append(False)\n",
    "                completion[model][\"hypothesis\"].append(False)\n",
    "                completion[model][\"experiment\"].append(False)\n",
    "                completion[model][\"analysis\"].append(False)\n",
    "                completion[model][\"improvement\"].append(False)\n",
    "                completion[model][\"summary\"].append(False)\n",
    "                completion[model][\"total\"].append(False)\n",
    "                continue\n",
    "                \n",
    "            #------------\n",
    "            # hypothesis\n",
    "            #------------\n",
    "            if ce_output.ce_cycle.hypothesis is not None:\n",
    "                hypothesis_input_tokens, hypothesis_output_tokens, hypothesis_total_tokens = add_metrics(\"hypothesis\")\n",
    "            else:\n",
    "                completion[model][\"experiment\"].append(False)\n",
    "                completion[model][\"analysis\"].append(False)\n",
    "                completion[model][\"improvement\"].append(False)\n",
    "                completion[model][\"hypothesis\"].append(False)\n",
    "                completion[model][\"summary\"].append(False)\n",
    "                completion[model][\"total\"].append(False)\n",
    "                continue\n",
    "    \n",
    "            #------------------\n",
    "            # Improvement loop\n",
    "            #------------------  \n",
    "            if ce_output.ce_cycle.completes_reconfig:\n",
    "                #------------\n",
    "                # experiment\n",
    "                #------------\n",
    "                exp_input_tokens, exp_output_tokens, exp_total_tokens = add_metrics(\"experiment\")\n",
    "                \n",
    "                #----------\n",
    "                # analysis\n",
    "                #----------\n",
    "                if len(ce_output.ce_cycle.analysis_history) > 0:\n",
    "                    analysis_input_tokens, analysis_output_tokens, analysis_total_tokens = add_metrics(\"analysis\")\n",
    "                else:\n",
    "                    completion[model][\"analysis\"].append(True)\n",
    "                    analysis_input_tokens, analysis_output_tokens, analysis_total_tokens = 0, 0, 0\n",
    "                \n",
    "                #-------------\n",
    "                # improvement\n",
    "                #-------------    \n",
    "                if len(ce_output.ce_cycle.reconfig_history) > 0:\n",
    "                    improve_input_tokens, improve_output_tokens, improve_total_tokens = add_metrics(\"improvement\")\n",
    "                else:\n",
    "                    completion[model][\"improvement\"].append(True)\n",
    "                    improve_input_tokens, improve_output_tokens, improve_total_tokens = 0, 0, 0\n",
    "    \n",
    "                #---------------\n",
    "                # reconfig rate\n",
    "                #---------------\n",
    "                if ce_output.ce_cycle.conducts_reconfig:\n",
    "                    reconfig_rate[model].append(True)\n",
    "                else:\n",
    "                    reconfig_rate[model].append(False)\n",
    "            else:\n",
    "                completion[model][\"experiment\"].append(False)\n",
    "                completion[model][\"analysis\"].append(False)\n",
    "                completion[model][\"improvement\"].append(False)\n",
    "                completion[model][\"summary\"].append(False)\n",
    "                completion[model][\"total\"].append(False)\n",
    "                continue\n",
    "    \n",
    "            \n",
    "            #----------------\n",
    "            # postprocessing\n",
    "            #----------------\n",
    "            if ce_output.ce_cycle.summary != \"\":\n",
    "                summary_input_tokens, summary_output_tokens, summary_total_tokens = add_metrics(\"summary\")\n",
    "            else:\n",
    "                completion[model][\"summary\"].append(False)\n",
    "                completion[model][\"total\"].append(False)\n",
    "                continue\n",
    "    \n",
    "            #-------\n",
    "            # total\n",
    "            #-------\n",
    "            if \"cycle\" in ce_output.run_time:\n",
    "                completion[model][\"total\"].append(True)\n",
    "                if ce_output.ce_cycle.conducts_reconfig:\n",
    "                    run_time[model][\"total\"].append(ce_output.run_time[\"cycle\"])\n",
    "                    input_tokens[model][\"total\"].append(preprocess_input_tokens + hypothesis_input_tokens + exp_input_tokens + analysis_input_tokens + improve_input_tokens + summary_input_tokens)\n",
    "                    output_tokens[model][\"total\"].append(preprocess_output_tokens + hypothesis_output_tokens + exp_output_tokens + analysis_output_tokens + improve_output_tokens + summary_output_tokens)\n",
    "                    total_tokens[model][\"total\"].append(preprocess_total_tokens + hypothesis_total_tokens + exp_total_tokens + analysis_total_tokens + improve_total_tokens + summary_total_tokens)\n",
    "            else:\n",
    "                completion[model][\"total\"].append(False)\n",
    "    \n",
    "            #-----------------\n",
    "            # reviewing score\n",
    "            #-----------------\n",
    "            # result_number = extract_number(os.path.basename(path))\n",
    "            # hypothesis_scores = []\n",
    "            # experiment_scores = []\n",
    "            # analysis_scores = []\n",
    "            # improvement_scores = []\n",
    "            # overall_scores = []\n",
    "            # for reviewer in REVIEWERS:\n",
    "            #     review_path = f\"{result_dir}/review{result_number}_{reviewer.split('/')[-1]}.json\"\n",
    "            #     review = load_json(review_path)\n",
    "            #     hypothesis_scores.append(review[\"hypothesis\"][\"score\"])\n",
    "            #     experiment_scores.append(review[\"experiment\"][\"score\"])\n",
    "            #     analysis_scores.append(review[\"analysis\"][\"score\"])\n",
    "            #     improvement_scores.append(review[\"improvement\"][\"score\"])\n",
    "            #     overall_scores.append(review[\"overall\"][\"score\"])\n",
    "            # score[model][\"hypothesis\"].append(hypothesis_scores)\n",
    "            # score[model][\"experiment\"].append(experiment_scores)\n",
    "            # score[model][\"analysis\"].append(analysis_scores)\n",
    "            # score[model][\"improvement\"].append(improvement_scores)\n",
    "            # score[model][\"total\"].append(overall_scores)\n",
    "    print(\"sample:\", sample)\n",
    "    print(\"input_tokens\", input_tokens)\n",
    "    print(\"output_tokens\", output_tokens)\n",
    "    print(\"total_tokens\", total_tokens)\n",
    "    print(\"run_time\", run_time)\n",
    "    print(\"completion\", completion)\n",
    "    print(\"review\", score)\n",
    "    print(\"reconfig_rate\", reconfig_rate)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be1ed107-b7f2-422e-aa8c-724070d38e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phase</th>\n",
       "      <th>Model</th>\n",
       "      <th>Time</th>\n",
       "      <th>Tokens ($)</th>\n",
       "      <th>Completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total</td>\n",
       "      <td>gpt-4o-2024-08-06</td>\n",
       "      <td>24.782558</td>\n",
       "      <td>283.9565/13.18 (0.842)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Preprocess</td>\n",
       "      <td>gpt-4o-2024-08-06</td>\n",
       "      <td>4.555809</td>\n",
       "      <td>30.1676/5.6662 (0.132)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hypothesis</td>\n",
       "      <td>gpt-4o-2024-08-06</td>\n",
       "      <td>4.267725</td>\n",
       "      <td>149.6586/3.836 (0.413)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Experiment</td>\n",
       "      <td>gpt-4o-2024-08-06</td>\n",
       "      <td>3.322877</td>\n",
       "      <td>57.233599999999996/1.7884 (0.161)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Analysis</td>\n",
       "      <td>gpt-4o-2024-08-06</td>\n",
       "      <td>35.700450</td>\n",
       "      <td>14.135/0.65225 (0.042)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Improvement</td>\n",
       "      <td>gpt-4o-2024-08-06</td>\n",
       "      <td>4.343703</td>\n",
       "      <td>15.24/0.6065 (0.044)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Summary</td>\n",
       "      <td>gpt-4o-2024-08-06</td>\n",
       "      <td>20.562414</td>\n",
       "      <td>18.177799999999998/0.4858 (0.050)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Phase              Model       Time  \\\n",
       "0        Total  gpt-4o-2024-08-06  24.782558   \n",
       "1   Preprocess  gpt-4o-2024-08-06   4.555809   \n",
       "2   Hypothesis  gpt-4o-2024-08-06   4.267725   \n",
       "3   Experiment  gpt-4o-2024-08-06   3.322877   \n",
       "4     Analysis  gpt-4o-2024-08-06  35.700450   \n",
       "5  Improvement  gpt-4o-2024-08-06   4.343703   \n",
       "6      Summary  gpt-4o-2024-08-06  20.562414   \n",
       "\n",
       "                          Tokens ($)  Completion  \n",
       "0             283.9565/13.18 (0.842)           1  \n",
       "1             30.1676/5.6662 (0.132)           1  \n",
       "2             149.6586/3.836 (0.413)           1  \n",
       "3  57.233599999999996/1.7884 (0.161)           1  \n",
       "4             14.135/0.65225 (0.042)           1  \n",
       "5               15.24/0.6065 (0.044)           1  \n",
       "6  18.177799999999998/0.4858 (0.050)           1  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statistics\n",
    "import itertools\n",
    "from chaos_eater.utils.llms import PRICING_PER_TOKEN\n",
    "\n",
    "def mean_list(lst: list) -> float:\n",
    "    if len(lst) > 0:\n",
    "        if isinstance(lst[0], list):\n",
    "            return statistics.mean(list(itertools.chain.from_iterable(lst)))\n",
    "        else:\n",
    "            return statistics.mean(lst)\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "rows = []\n",
    "for phase in [\"total\", \"preprocess\", \"hypothesis\", \"experiment\", \"analysis\", \"improvement\", \"summary\"]:\n",
    "    for model in MODELS:\n",
    "        run_time_mean = mean_list(run_time[model][phase])\n",
    "        input_tokens_mean = mean_list(input_tokens[model][phase])\n",
    "        output_tokens_mean = mean_list(output_tokens[model][phase])\n",
    "        billing_mean = PRICING_PER_TOKEN[model][\"input\"] * input_tokens_mean + PRICING_PER_TOKEN[model][\"output\"] * output_tokens_mean\n",
    "        completion_mean = mean_list(completion[model][phase])\n",
    "        score_mean = mean_list(score[model][phase])\n",
    "        row = {\n",
    "            \"Phase\": phase.capitalize(),\n",
    "            \"Model\": model.split('/')[-1],\n",
    "            \"Time\": run_time_mean if run_time_mean <= 60 else run_time_mean/60,\n",
    "            \"Tokens ($)\": f\"{input_tokens_mean/1000}/{output_tokens_mean/1000} ({billing_mean:.3f})\",\n",
    "            \"Completion\": completion_mean,\n",
    "        }\n",
    "        rows.append(row)\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
