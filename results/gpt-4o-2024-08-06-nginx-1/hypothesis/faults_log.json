[
    {
        "name": "fault_scenario_assumption",
        "token_usage": {
            "input_tokens": 2891,
            "output_tokens": 281,
            "total_tokens": 3172
        },
        "message_history": [
            [
                "System: You are a helpful AI assistant for Chaos Engineering. \nGiven k8s manifests for a system, the steady states of the system, and user's instructions for Chaos Engineering, you will define the most impactful fault injections to reveal potential weaknesses of the system, such as insufficient recovery functions, resource allocation, redundancy, etc.\nAlways keep the following rules:\n- First, assume a real-world event that may be most impactful in the the system, such as promotion campaign, cyber attacks, disasters, etc.\n- Then, define the most impactful fault injections to reveal potential weaknesses of the given system while simulating the assumed real-world event.\n- Prioritize fault injections that target the system's weak resources related to the steady states to verify whether those resources can handle the faults and the steady states can be maintained.\n- The injected faults should be selected from the following fault types of Chaos Mesh:\n- PodChaos: simulates Pod failures, such as Pod node restart, Pod's persistent unavailablility, and certain container failures in a specific Pod. The supported subtypes include 'pod-failure', 'pod-kill', 'container-kill'.\n- NetworkChaos: simulates network failures, such as network latency, packet loss, packet disorder, and network partitions.\n- DNSChaos: simulates DNS failures, such as the parsing failure of DNS domain name and the wrong IP address returned.\n- HTTPChaos: simulates HTTP communication failures, such as HTTP communication latency.\n- StressChaos: simulates CPU race or memory race.\n- IOChaos: simulates the I/O failure of an application file, such as I/O delays, read and write failures.\n- TimeChaos: simulates the time jump exception.\n\n- The output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n\nHere is the output schema:\n```\n{\"properties\": {\"event\": {\"title\": \"Event\", \"description\": \"Consider a real-world fault event that may be most impactful of the system, such as promotion campaign, cyber attacks, disasters, etc.\", \"type\": \"string\"}, \"thought\": {\"title\": \"Thought\", \"description\": \"Write down your thought process to define a sequence of fault injections that exploit the system's weaknesses of while simulating the fault event: 1) how the system's weaknesses affect the steady state; 2) how each fault injection exploit the system's weaknesses; 3) how the sequence simulates the phenamena in the fault event (consider carefully the sequence order). Prioritize fault injections that directly attack the weaknessses of the system, such as insufficient recovery functions, resource allocation, redundancy, etc.\", \"type\": \"string\"}, \"faults\": {\"title\": \"Faults\", \"description\": \"Define a sequence of fault injections that exploit the system's vulnerabilities to the fullest according to the above thoughts. In the inner list, a set of simultaneously injected faults are listed, while in the outer list, the sets are listed in the injection order. For example, [[fault_a], [fault_b, fault_c]] indicates that fault_a is injected, then fault_b and fault_c are injected simultaneously.\", \"type\": \"array\", \"items\": {\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Fault\"}}}}, \"required\": [\"event\", \"thought\", \"faults\"], \"definitions\": {\"Fault\": {\"title\": \"Fault\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"Select a fault type from [\\\"PodChaos\\\", \\\"NetworkChaos\\\", \\\"DNSChaos\\\", \\\"HTTPChaos\\\", \\\"StressChaos\\\", \\\"IOChaos\\\", \\\"TimeChaos\\\"]\", \"enum\": [\"PodChaos\", \"NetworkChaos\", \"DNSChaos\", \"HTTPChaos\", \"StressChaos\", \"IOChaos\", \"TimeChaos\"], \"type\": \"string\"}, \"name_id\": {\"title\": \"Name Id\", \"description\": \"An identifier to prevent name conflicts when the same Fault appears. Assign numbers starting from 0 in sequential order to prevent name conflicts.\", \"type\": \"integer\"}, \"scope\": {\"title\": \"Scope\", \"description\": \"Specify only the fault injection scope (i.e., the target resource where the fault is injected) in advance here.\", \"type\": \"object\", \"additionalProperties\": {\"type\": \"string\"}}}, \"required\": [\"name\", \"name_id\", \"scope\"]}}}\n```\nHuman: Here is the overview of my system:\nThe system consists of the following K8s manifest(s):K8s manifest: nginx/pod.yaml\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example-pod\n  labels:\n    app: example\nspec:\n  restartPolicy: Never\n  containers:\n  - name: example-container\n    image: nginx:1.17.1\n    ports:\n    - containerPort: 80\n```\nSummary of nginx/pod.yaml:\n- This manifest defines a Kubernetes Pod.\n- The Pod is named 'example-pod'.\n- It includes metadata with a label 'app: example'.\n- The Pod's restart policy is set to 'Never', meaning it won't restart automatically if it fails.\n- The Pod contains a single container named 'example-container'.\n- The container uses the 'nginx:1.17.1' image.\n- The container exposes port 80, which is typically used for HTTP traffic.\n\nK8s manifest: nginx/service.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: example-service\nspec:\n  selector:\n    app: example\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n```\nSummary of nginx/service.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'example-service'.\n- It uses a selector to target pods with the label 'app: example'.\n- The Service listens on port 80 using the TCP protocol.\n- It forwards traffic to the target port 80 on the selected pods.\n\nThe resiliency issues/weaknesses in the system are as follows:\nIssue #0: Pod Restart Policy\n  - details: The Pod will not restart automatically if it fails, which can lead to downtime.\n  - manifests having the issues: ['nginx/pod.yaml']\n  - problematic config: restartPolicy: Never\n\nIssue #1: Single Pod Deployment\n  - details: Using a single Pod without a controller like Deployment or ReplicaSet can lead to lack of redundancy and no automatic recovery if the Pod is deleted or fails.\n  - manifests having the issues: ['nginx/pod.yaml']\n  - problematic config: kind: Pod\n\nThe expected type of application on the system (i.e., K8s manfests):\nWeb server hosting a static website using Nginx.; The manifests provided are for a Pod and a Service in Kubernetes, both of which are associated with the label 'app: example'. The Pod runs an Nginx container, which is a popular web server and reverse proxy server. The Service is configured to expose the Pod on port 80, which is the default port for HTTP traffic. Given these details, it is logical to assume that the application is a simple web server or a static website hosted using Nginx. The use of Nginx and the exposure of port 80 strongly suggest that the application is intended to serve web content.\n\nSteady states of the network system defined by the manifests are the following:\n2 steady states are defined.\n1st steady states:\n- Name: PodRunningStatusNginxPodYaml\n- Description: The first issue to address is the Pod's restart policy set to 'Never'. This is a critical issue because if the Pod fails, it will not restart automatically, leading to potential downtime. Therefore, the steady state should verify that the Pod is running and not in a failed state. This can be measured by checking the Pod's status to ensure it is 'Running'. This steady state is specific to the 'nginx/pod.yaml' manifest, which defines the Pod with the problematic restart policy.\n- Threshold for the steady state: The Pod should be in the 'Running' state for at least 90% of the time during the observation period.; The steady state we are considering is the Pod's running status. The current state shows that the Pod was running for 5 out of 5 seconds, which is 100% of the time. Given that the Pod's restart policy is set to 'Never', it is crucial to ensure that the Pod remains in the 'Running' state to avoid downtime. However, to account for minor fluctuations or delays in status updates, a reasonable threshold would be to expect the Pod to be in the 'Running' state for at least 90% of the time during the observation period. This allows for some tolerance while still ensuring that the Pod is generally stable and operational.\n- Whether the steady state meets the threshold is determined by the following Python script with K8s API:\n```\nimport os\nimport time\nimport argparse\nfrom kubernetes import client, config\nfrom unittest_base import K8sAPIBase\n\nclass TestPodRunningStatus(K8sAPIBase):\n    def __init__(self, namespace, pod_name, duration):\n        super().__init__()\n        self.namespace = namespace\n        self.pod_name = pod_name\n        self.duration = duration\n\n    def test_pod_running_status(self):\n        running_count = 0\n        # Loop for the specified duration\n        for _ in range(self.duration):\n            try:\n                # Read the Pod status\n                pod = self.v1.read_namespaced_pod(name=self.pod_name, namespace=self.namespace)\n                # Check if the Pod is in 'Running' state\n                if pod.status.phase == 'Running':\n                    running_count += 1\n                print(f\"Pod status: {pod.status.phase}\")\n            except client.exceptions.ApiException as e:\n                print(f\"Exception when calling CoreV1Api->read_namespaced_pod: {e}\")\n            time.sleep(1)\n        # Calculate the percentage of time the Pod was running\n        running_percentage = (running_count / self.duration) * 100\n        print(f\"Pod was running {running_count} out of {self.duration} seconds, which is {running_percentage}% of the time.\")\n        # Assert that the Pod was running at least 90% of the time\n        assert running_percentage >= 90, \"Pod did not meet the 90% running threshold.\"\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Test the running status of a Kubernetes Pod.')\n    parser.add_argument('--duration', type=int, default=5, help='Duration to check the Pod status in seconds.')\n    args = parser.parse_args()\n    # Create an instance of the test class\n    test = TestPodRunningStatus(namespace='default', pod_name='example-pod', duration=args.duration)\n    # Run the test\n    test.test_pod_running_status()\n```2nd steady states:\n- Name: ServiceAvailabilityNginxServiceYaml\n- Description: The next issue to address is the lack of redundancy due to the use of a single Pod without a controller like Deployment or ReplicaSet. This can lead to a lack of automatic recovery if the Pod is deleted or fails. To verify the system's resilience, we should define a steady state that measures the availability of the web service provided by the Pod. Since the Service is responsible for exposing the Pod, we can measure the HTTP response rate or availability of the service. This steady state will ensure that the Service is consistently able to route traffic to the Pod, even if the Pod encounters issues. This is specific to the 'nginx/service.yaml' manifest, which defines the Service that selects the Pod.\n- Threshold for the steady state: HTTP response rate should be >= 99.5% with status code 200.; The steady state we are considering is the availability of the web service provided by the Nginx Pod, as measured by the HTTP response rate. The k6 test results show that 100% of the requests received a 200 status code, indicating that the service is fully available. To account for potential fluctuations and ensure the threshold is reasonable, we should allow for a small margin of error. A typical approach is to set the threshold slightly below 100% to accommodate minor network issues or transient errors that might occur in a real-world scenario. Given the current state shows perfect availability, a threshold of 99.5% would be reasonable, allowing for some tolerance while still ensuring high availability.\n- Whether the steady state meets the threshold is determined by the following K6 Javascript:\n```\nimport http from 'k6/http';\nimport { check } from 'k6';\n\nexport let options = {\n  vus: 10,\n  duration: '5s',\n  thresholds: {\n    'http_req_failed': ['rate<0.005'], // Allowing for a 0.5% failure rate\n    'http_req_duration': ['p(95)<200'], // Optional: 95% of requests should be below 200ms\n  },\n};\n\nexport default function () {\n  let res = http.get('http://example-service.default.svc.cluster.local:80');\n  check(res, {\n    'status is 200': (r) => r.status === 200,\n  });\n}\n```\n\nPlease follow the instructions below regarding Chaos Engineering as necessary:\n- The Chaos-Engineering experiment must be completed within 1 minute.\n\nNow, please define fault injections to reveal the system's vulnerabilities.\nAI: ```json\n{\"event\":"
            ],
            "{\"event\": \"Cyber Attack\", \"thought\": \"In a cyber attack scenario, the attacker might target the single point of failure in the system, which is the single Pod running the Nginx server. The Pod's restart policy is set to 'Never', meaning if it fails, it won't restart automatically, leading to downtime. Additionally, the lack of redundancy due to the single Pod deployment without a controller like Deployment or ReplicaSet makes the system vulnerable to attacks that can take down the Pod. To simulate this, we can inject a PodChaos fault to kill the Pod, testing the system's ability to maintain the steady state of service availability. This will directly test the system's weakness in handling Pod failures and its impact on the steady state of service availability. Following this, we can simulate a network partition using NetworkChaos to test if the service can still route traffic effectively, even if the Pod is isolated from the network. This sequence simulates a cyber attack where the attacker first takes down the Pod and then attempts to disrupt network communication.\", \"faults\": [[{\"name\": \"PodChaos\", \"name_id\": 0, \"scope\": {\"namespace\": \"default\", \"pod_name\": \"example-pod\"}}], [{\"name\": \"NetworkChaos\", \"name_id\": 1, \"scope\": {\"namespace\": \"default\", \"pod_name\": \"example-pod\"}}]]}\n"
        ]
    },
    {
        "name": "refine_fault_params",
        "token_usage": {
            "input_tokens": 9278,
            "output_tokens": 74,
            "total_tokens": 9352
        },
        "message_history": [
            [
                "System: You are a helpful AI assistant for Chaos Engineering.\nGiven k8s manifests that defines a network system, its steady states, and a fault type that may affect the steady states in the system, please detail the parameters of the fault.\nAlways keep the following rules:\n- Pay attention to namespace specification. If the namespace is specified in the manifest, it is deployed with the namespace. If not, it is deployed with the 'default' namespace.\n- The parameters follow the format of Chaos Mesh.\nHuman: Here is the overview of my system:\nThe system consists of the following K8s manifest(s):K8s manifest: nginx/pod.yaml\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example-pod\n  labels:\n    app: example\nspec:\n  restartPolicy: Never\n  containers:\n  - name: example-container\n    image: nginx:1.17.1\n    ports:\n    - containerPort: 80\n```\nSummary of nginx/pod.yaml:\n- This manifest defines a Kubernetes Pod.\n- The Pod is named 'example-pod'.\n- It includes metadata with a label 'app: example'.\n- The Pod's restart policy is set to 'Never', meaning it won't restart automatically if it fails.\n- The Pod contains a single container named 'example-container'.\n- The container uses the 'nginx:1.17.1' image.\n- The container exposes port 80, which is typically used for HTTP traffic.\n\nK8s manifest: nginx/service.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: example-service\nspec:\n  selector:\n    app: example\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n```\nSummary of nginx/service.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'example-service'.\n- It uses a selector to target pods with the label 'app: example'.\n- The Service listens on port 80 using the TCP protocol.\n- It forwards traffic to the target port 80 on the selected pods.\n\nThe resiliency issues/weaknesses in the system are as follows:\nIssue #0: Pod Restart Policy\n  - details: The Pod will not restart automatically if it fails, which can lead to downtime.\n  - manifests having the issues: ['nginx/pod.yaml']\n  - problematic config: restartPolicy: Never\n\nIssue #1: Single Pod Deployment\n  - details: Using a single Pod without a controller like Deployment or ReplicaSet can lead to lack of redundancy and no automatic recovery if the Pod is deleted or fails.\n  - manifests having the issues: ['nginx/pod.yaml']\n  - problematic config: kind: Pod\n\nThe expected type of application on the system (i.e., K8s manfests):\nWeb server hosting a static website using Nginx.; The manifests provided are for a Pod and a Service in Kubernetes, both of which are associated with the label 'app: example'. The Pod runs an Nginx container, which is a popular web server and reverse proxy server. The Service is configured to expose the Pod on port 80, which is the default port for HTTP traffic. Given these details, it is logical to assume that the application is a simple web server or a static website hosted using Nginx. The use of Nginx and the exposure of port 80 strongly suggest that the application is intended to serve web content.\n\nSteady states of my system:\n2 steady states are defined.\n1st steady states:\n- Name: PodRunningStatusNginxPodYaml\n- Description: The first issue to address is the Pod's restart policy set to 'Never'. This is a critical issue because if the Pod fails, it will not restart automatically, leading to potential downtime. Therefore, the steady state should verify that the Pod is running and not in a failed state. This can be measured by checking the Pod's status to ensure it is 'Running'. This steady state is specific to the 'nginx/pod.yaml' manifest, which defines the Pod with the problematic restart policy.\n- Threshold for the steady state: The Pod should be in the 'Running' state for at least 90% of the time during the observation period.; The steady state we are considering is the Pod's running status. The current state shows that the Pod was running for 5 out of 5 seconds, which is 100% of the time. Given that the Pod's restart policy is set to 'Never', it is crucial to ensure that the Pod remains in the 'Running' state to avoid downtime. However, to account for minor fluctuations or delays in status updates, a reasonable threshold would be to expect the Pod to be in the 'Running' state for at least 90% of the time during the observation period. This allows for some tolerance while still ensuring that the Pod is generally stable and operational.\n- Whether the steady state meets the threshold is determined by the following Python script with K8s API:\n```\nimport os\nimport time\nimport argparse\nfrom kubernetes import client, config\nfrom unittest_base import K8sAPIBase\n\nclass TestPodRunningStatus(K8sAPIBase):\n    def __init__(self, namespace, pod_name, duration):\n        super().__init__()\n        self.namespace = namespace\n        self.pod_name = pod_name\n        self.duration = duration\n\n    def test_pod_running_status(self):\n        running_count = 0\n        # Loop for the specified duration\n        for _ in range(self.duration):\n            try:\n                # Read the Pod status\n                pod = self.v1.read_namespaced_pod(name=self.pod_name, namespace=self.namespace)\n                # Check if the Pod is in 'Running' state\n                if pod.status.phase == 'Running':\n                    running_count += 1\n                print(f\"Pod status: {pod.status.phase}\")\n            except client.exceptions.ApiException as e:\n                print(f\"Exception when calling CoreV1Api->read_namespaced_pod: {e}\")\n            time.sleep(1)\n        # Calculate the percentage of time the Pod was running\n        running_percentage = (running_count / self.duration) * 100\n        print(f\"Pod was running {running_count} out of {self.duration} seconds, which is {running_percentage}% of the time.\")\n        # Assert that the Pod was running at least 90% of the time\n        assert running_percentage >= 90, \"Pod did not meet the 90% running threshold.\"\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Test the running status of a Kubernetes Pod.')\n    parser.add_argument('--duration', type=int, default=5, help='Duration to check the Pod status in seconds.')\n    args = parser.parse_args()\n    # Create an instance of the test class\n    test = TestPodRunningStatus(namespace='default', pod_name='example-pod', duration=args.duration)\n    # Run the test\n    test.test_pod_running_status()\n```2nd steady states:\n- Name: ServiceAvailabilityNginxServiceYaml\n- Description: The next issue to address is the lack of redundancy due to the use of a single Pod without a controller like Deployment or ReplicaSet. This can lead to a lack of automatic recovery if the Pod is deleted or fails. To verify the system's resilience, we should define a steady state that measures the availability of the web service provided by the Pod. Since the Service is responsible for exposing the Pod, we can measure the HTTP response rate or availability of the service. This steady state will ensure that the Service is consistently able to route traffic to the Pod, even if the Pod encounters issues. This is specific to the 'nginx/service.yaml' manifest, which defines the Service that selects the Pod.\n- Threshold for the steady state: HTTP response rate should be >= 99.5% with status code 200.; The steady state we are considering is the availability of the web service provided by the Nginx Pod, as measured by the HTTP response rate. The k6 test results show that 100% of the requests received a 200 status code, indicating that the service is fully available. To account for potential fluctuations and ensure the threshold is reasonable, we should allow for a small margin of error. A typical approach is to set the threshold slightly below 100% to accommodate minor network issues or transient errors that might occur in a real-world scenario. Given the current state shows perfect availability, a threshold of 99.5% would be reasonable, allowing for some tolerance while still ensuring high availability.\n- Whether the steady state meets the threshold is determined by the following K6 Javascript:\n```\nimport http from 'k6/http';\nimport { check } from 'k6';\n\nexport let options = {\n  vus: 10,\n  duration: '5s',\n  thresholds: {\n    'http_req_failed': ['rate<0.005'], // Allowing for a 0.5% failure rate\n    'http_req_duration': ['p(95)<200'], // Optional: 95% of requests should be below 200ms\n  },\n};\n\nexport default function () {\n  let res = http.get('http://example-service.default.svc.cluster.local:80');\n  check(res, {\n    'status is 200': (r) => r.status === 200,\n  });\n}\n```\n\nA fault scenario that may occur in my system and may affect the steady states:\n\nAn assumed fault scenario is as follows:\n- Event: Cyber Attack\n- Used Chaos Engineering tool: Chaos Mesh\n- Faults to simulate the event: [[{'name': 'PodChaos', 'name_id': 0, 'scope': {'namespace': 'default', 'pod_name': 'example-pod'}}], [{'name': 'NetworkChaos', 'name_id': 1, 'scope': {'namespace': 'default', 'pod_name': 'example-pod'}}]]\n- Description: In a cyber attack scenario, the attacker might target the single point of failure in the system, which is the single Pod running the Nginx server. The Pod's restart policy is set to 'Never', meaning if it fails, it won't restart automatically, leading to downtime. Additionally, the lack of redundancy due to the single Pod deployment without a controller like Deployment or ReplicaSet makes the system vulnerable to attacks that can take down the Pod. To simulate this, we can inject a PodChaos fault to kill the Pod, testing the system's ability to maintain the steady state of service availability. This will directly test the system's weakness in handling Pod failures and its impact on the steady state of service availability. Following this, we can simulate a network partition using NetworkChaos to test if the service can still route traffic effectively, even if the Pod is isolated from the network. This sequence simulates a cyber attack where the attacker first takes down the Pod and then attempts to disrupt network communication.\n\nPlease follow the instructions below regarding Chaos Engineering as necessary:\n- The Chaos-Engineering experiment must be completed within 1 minute.\n\nNow, please detail the parameters of the fault \"PodChaos({'namespace': 'default', 'pod_name': 'example-pod'})\".\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n\nHere is the output schema:\n```\n{\"properties\": {\"action\": {\"title\": \"Action\", \"description\": \"Specifies the fault type from 'pod-failure', 'pod-kill', or 'container-kill'. Note that you may select 'pod-failure' only when the target Pod's container has livenessProbe and readinessProbe defined.\", \"example\": \"pod-kill\", \"enum\": [\"pod-failure\", \"pod-kill\", \"container-kill\"], \"type\": \"string\"}, \"mode\": {\"title\": \"Mode\", \"description\": \"Specifies the mode of the experiment. The mode options include 'one' (selecting a random Pod), 'all' (selecting all eligible Pods), 'fixed' (selecting a specified number of eligible Pods), 'fixed-percent' (selecting a specified percentage of Pods from the eligible Pods), and 'random-max-percent' (selecting the maximum percentage of Pods from the eligible Pods)\", \"example\": \"one\", \"enum\": [\"one\", \"all\", \"fixed\", \"fixed-percent\", \"random-max-percent\"], \"type\": \"string\"}, \"value\": {\"title\": \"Value\", \"description\": \"Provides parameters for the mode configuration, depending on mode.For example, when mode is set to fixed-percent, value specifies the percentage of Pods.\", \"example\": \"1\", \"type\": \"string\"}, \"selector\": {\"title\": \"Selector\", \"description\": \"Specifies the target Pod.\", \"example\": null, \"allOf\": [{\"$ref\": \"#/definitions/Selectors\"}]}, \"containerNames\": {\"title\": \"Containernames\", \"description\": \"When you configure action to container-kill, this configuration is mandatory to specify the target container name for injecting faults.\", \"example\": [\"prometheus\"], \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"action\", \"mode\", \"selector\"], \"definitions\": {\"SetBasedRequirements\": {\"title\": \"SetBasedRequirements\", \"type\": \"object\", \"properties\": {\"key\": {\"title\": \"Key\", \"description\": \"Label key\", \"type\": \"string\"}, \"operator\": {\"title\": \"Operator\", \"description\": \"Select an operator.\", \"enum\": [\"In\", \"NotIn\", \"Exists\", \"DoesNotExist\"], \"type\": \"string\"}, \"values\": {\"title\": \"Values\", \"description\": \"Label values. The values set must be non-empty in the case of In and NotIn.\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"key\", \"operator\", \"values\"]}, \"Selectors\": {\"title\": \"Selectors\", \"type\": \"object\", \"properties\": {\"namespaces\": {\"title\": \"Namespaces\", \"description\": \"Specifies the namespace of the experiment's target Pod. If this selector is None, Chaos Mesh will set it to the namespace of the current Chaos experiment.\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}, \"labelSelectors\": {\"title\": \"Labelselectors\", \"description\": \"Specifies the label-key/value pairs that the experiment's target Pod must have. If multiple labels are specified, the experiment target must have all the labels specified by this selector.\", \"type\": \"object\", \"additionalProperties\": {\"type\": \"string\"}}, \"expressionSelectors\": {\"title\": \"Expressionselectors\", \"description\": \"Specifies a set of expressions that define the label's rules to specifiy the experiment's target Pod.\", \"example\": [{\"key\": \"tier\", \"operator\": \"In\", \"values\": [\"cache\"]}, {\"key\": \"environment\", \"operator\": \"NotIn\", \"values\": [\"dev\"]}], \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/SetBasedRequirements\"}}, \"annotationSelectors\": {\"title\": \"Annotationselectors\", \"description\": \"Specifies the annotation-key/value pairs that the experiment's target Pod must have. If multiple annotations are specified, the experiment target must have all annotations specified by this selector.\", \"type\": \"object\", \"additionalProperties\": {\"type\": \"string\"}}, \"fieldSelectors\": {\"title\": \"Fieldselectors\", \"description\": \"Specifies the field-key/value pairs of the experiment's target Pod. If multiple fields are specified, the experiment target must have all fields set by this selector.\", \"example\": {\"metadata.name\": \"my-pod\", \"metadata.namespace\": \"dafault\"}, \"type\": \"object\", \"additionalProperties\": {\"type\": \"string\"}}, \"podPhaseSelectors\": {\"title\": \"Podphaseselectors\", \"description\": \"Specifies the phase of the experiment's target Pod. If this selector is None, the target Pod's phase is not limited.\", \"type\": \"array\", \"items\": {\"enum\": [\"Pending\", \"Running\", \"Succeeded\", \"Failed\", \"Unknown\"], \"type\": \"string\"}}, \"nodeSelectors\": {\"title\": \"Nodeselectors\", \"description\": \"Specifies the node-label-key/value pairs to which the experiment's target Pod belongs.\", \"type\": \"object\", \"additionalProperties\": {\"type\": \"string\"}}, \"nodes\": {\"title\": \"Nodes\", \"description\": \"Specifies the node to which the experiment's target Pod belongs. The target Pod can only belong to one node in the configured node list. If multiple node labels are specified, the node to which the experiment's target Pod belongs must have all labels specified by this selector.\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}, \"pods\": {\"title\": \"Pods\", \"description\": \"Specifies the namespaces and list of the experiment's target Pods. If you have specified this selector, Chaos Mesh ignores other configured selectors.\", \"example\": {\"default\": [\"pod-0\", \"pod-2\"]}, \"type\": \"object\", \"additionalProperties\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}}}}}}\n```\nAI: ```json\n{\"action\":"
            ],
            "{\"action\": \"pod-kill\", \"mode\": \"one\", \"selector\": {\"namespaces\": [\"default\"], \"labelSelectors\": {\"app\": \"example\"}}}",
            [
                "System: You are a helpful AI assistant for Chaos Engineering.\nGiven k8s manifests that defines a network system, its steady states, and a fault type that may affect the steady states in the system, please detail the parameters of the fault.\nAlways keep the following rules:\n- Pay attention to namespace specification. If the namespace is specified in the manifest, it is deployed with the namespace. If not, it is deployed with the 'default' namespace.\n- The parameters follow the format of Chaos Mesh.\nHuman: Here is the overview of my system:\nThe system consists of the following K8s manifest(s):K8s manifest: nginx/pod.yaml\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example-pod\n  labels:\n    app: example\nspec:\n  restartPolicy: Never\n  containers:\n  - name: example-container\n    image: nginx:1.17.1\n    ports:\n    - containerPort: 80\n```\nSummary of nginx/pod.yaml:\n- This manifest defines a Kubernetes Pod.\n- The Pod is named 'example-pod'.\n- It includes metadata with a label 'app: example'.\n- The Pod's restart policy is set to 'Never', meaning it won't restart automatically if it fails.\n- The Pod contains a single container named 'example-container'.\n- The container uses the 'nginx:1.17.1' image.\n- The container exposes port 80, which is typically used for HTTP traffic.\n\nK8s manifest: nginx/service.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: example-service\nspec:\n  selector:\n    app: example\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n```\nSummary of nginx/service.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'example-service'.\n- It uses a selector to target pods with the label 'app: example'.\n- The Service listens on port 80 using the TCP protocol.\n- It forwards traffic to the target port 80 on the selected pods.\n\nThe resiliency issues/weaknesses in the system are as follows:\nIssue #0: Pod Restart Policy\n  - details: The Pod will not restart automatically if it fails, which can lead to downtime.\n  - manifests having the issues: ['nginx/pod.yaml']\n  - problematic config: restartPolicy: Never\n\nIssue #1: Single Pod Deployment\n  - details: Using a single Pod without a controller like Deployment or ReplicaSet can lead to lack of redundancy and no automatic recovery if the Pod is deleted or fails.\n  - manifests having the issues: ['nginx/pod.yaml']\n  - problematic config: kind: Pod\n\nThe expected type of application on the system (i.e., K8s manfests):\nWeb server hosting a static website using Nginx.; The manifests provided are for a Pod and a Service in Kubernetes, both of which are associated with the label 'app: example'. The Pod runs an Nginx container, which is a popular web server and reverse proxy server. The Service is configured to expose the Pod on port 80, which is the default port for HTTP traffic. Given these details, it is logical to assume that the application is a simple web server or a static website hosted using Nginx. The use of Nginx and the exposure of port 80 strongly suggest that the application is intended to serve web content.\n\nSteady states of my system:\n2 steady states are defined.\n1st steady states:\n- Name: PodRunningStatusNginxPodYaml\n- Description: The first issue to address is the Pod's restart policy set to 'Never'. This is a critical issue because if the Pod fails, it will not restart automatically, leading to potential downtime. Therefore, the steady state should verify that the Pod is running and not in a failed state. This can be measured by checking the Pod's status to ensure it is 'Running'. This steady state is specific to the 'nginx/pod.yaml' manifest, which defines the Pod with the problematic restart policy.\n- Threshold for the steady state: The Pod should be in the 'Running' state for at least 90% of the time during the observation period.; The steady state we are considering is the Pod's running status. The current state shows that the Pod was running for 5 out of 5 seconds, which is 100% of the time. Given that the Pod's restart policy is set to 'Never', it is crucial to ensure that the Pod remains in the 'Running' state to avoid downtime. However, to account for minor fluctuations or delays in status updates, a reasonable threshold would be to expect the Pod to be in the 'Running' state for at least 90% of the time during the observation period. This allows for some tolerance while still ensuring that the Pod is generally stable and operational.\n- Whether the steady state meets the threshold is determined by the following Python script with K8s API:\n```\nimport os\nimport time\nimport argparse\nfrom kubernetes import client, config\nfrom unittest_base import K8sAPIBase\n\nclass TestPodRunningStatus(K8sAPIBase):\n    def __init__(self, namespace, pod_name, duration):\n        super().__init__()\n        self.namespace = namespace\n        self.pod_name = pod_name\n        self.duration = duration\n\n    def test_pod_running_status(self):\n        running_count = 0\n        # Loop for the specified duration\n        for _ in range(self.duration):\n            try:\n                # Read the Pod status\n                pod = self.v1.read_namespaced_pod(name=self.pod_name, namespace=self.namespace)\n                # Check if the Pod is in 'Running' state\n                if pod.status.phase == 'Running':\n                    running_count += 1\n                print(f\"Pod status: {pod.status.phase}\")\n            except client.exceptions.ApiException as e:\n                print(f\"Exception when calling CoreV1Api->read_namespaced_pod: {e}\")\n            time.sleep(1)\n        # Calculate the percentage of time the Pod was running\n        running_percentage = (running_count / self.duration) * 100\n        print(f\"Pod was running {running_count} out of {self.duration} seconds, which is {running_percentage}% of the time.\")\n        # Assert that the Pod was running at least 90% of the time\n        assert running_percentage >= 90, \"Pod did not meet the 90% running threshold.\"\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Test the running status of a Kubernetes Pod.')\n    parser.add_argument('--duration', type=int, default=5, help='Duration to check the Pod status in seconds.')\n    args = parser.parse_args()\n    # Create an instance of the test class\n    test = TestPodRunningStatus(namespace='default', pod_name='example-pod', duration=args.duration)\n    # Run the test\n    test.test_pod_running_status()\n```2nd steady states:\n- Name: ServiceAvailabilityNginxServiceYaml\n- Description: The next issue to address is the lack of redundancy due to the use of a single Pod without a controller like Deployment or ReplicaSet. This can lead to a lack of automatic recovery if the Pod is deleted or fails. To verify the system's resilience, we should define a steady state that measures the availability of the web service provided by the Pod. Since the Service is responsible for exposing the Pod, we can measure the HTTP response rate or availability of the service. This steady state will ensure that the Service is consistently able to route traffic to the Pod, even if the Pod encounters issues. This is specific to the 'nginx/service.yaml' manifest, which defines the Service that selects the Pod.\n- Threshold for the steady state: HTTP response rate should be >= 99.5% with status code 200.; The steady state we are considering is the availability of the web service provided by the Nginx Pod, as measured by the HTTP response rate. The k6 test results show that 100% of the requests received a 200 status code, indicating that the service is fully available. To account for potential fluctuations and ensure the threshold is reasonable, we should allow for a small margin of error. A typical approach is to set the threshold slightly below 100% to accommodate minor network issues or transient errors that might occur in a real-world scenario. Given the current state shows perfect availability, a threshold of 99.5% would be reasonable, allowing for some tolerance while still ensuring high availability.\n- Whether the steady state meets the threshold is determined by the following K6 Javascript:\n```\nimport http from 'k6/http';\nimport { check } from 'k6';\n\nexport let options = {\n  vus: 10,\n  duration: '5s',\n  thresholds: {\n    'http_req_failed': ['rate<0.005'], // Allowing for a 0.5% failure rate\n    'http_req_duration': ['p(95)<200'], // Optional: 95% of requests should be below 200ms\n  },\n};\n\nexport default function () {\n  let res = http.get('http://example-service.default.svc.cluster.local:80');\n  check(res, {\n    'status is 200': (r) => r.status === 200,\n  });\n}\n```\n\nA fault scenario that may occur in my system and may affect the steady states:\n\nAn assumed fault scenario is as follows:\n- Event: Cyber Attack\n- Used Chaos Engineering tool: Chaos Mesh\n- Faults to simulate the event: [[{'name': 'PodChaos', 'name_id': 0, 'scope': {'namespace': 'default', 'pod_name': 'example-pod'}}], [{'name': 'NetworkChaos', 'name_id': 1, 'scope': {'namespace': 'default', 'pod_name': 'example-pod'}}]]\n- Description: In a cyber attack scenario, the attacker might target the single point of failure in the system, which is the single Pod running the Nginx server. The Pod's restart policy is set to 'Never', meaning if it fails, it won't restart automatically, leading to downtime. Additionally, the lack of redundancy due to the single Pod deployment without a controller like Deployment or ReplicaSet makes the system vulnerable to attacks that can take down the Pod. To simulate this, we can inject a PodChaos fault to kill the Pod, testing the system's ability to maintain the steady state of service availability. This will directly test the system's weakness in handling Pod failures and its impact on the steady state of service availability. Following this, we can simulate a network partition using NetworkChaos to test if the service can still route traffic effectively, even if the Pod is isolated from the network. This sequence simulates a cyber attack where the attacker first takes down the Pod and then attempts to disrupt network communication.\n\nPlease follow the instructions below regarding Chaos Engineering as necessary:\n- The Chaos-Engineering experiment must be completed within 1 minute.\n\nNow, please detail the parameters of the fault \"NetworkChaos({'namespace': 'default', 'pod_name': 'example-pod'})\".\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n\nHere is the output schema:\n```\n{\"properties\": {\"action\": {\"title\": \"Action\", \"description\": \"Indicates the specific fault type. Available types include: netem, delay (network delay), loss (packet loss), duplicate (packet duplicating), corrupt (packet corrupt), partition (network partition), and bandwidth (network bandwidth limit). After you specify action field, specify action-related fields for other necessary field configuration.\", \"example\": \"Partition\", \"enum\": [\"netem\", \"delay\", \"loss\", \"duplicate\", \"corrupt\", \"partition\", \"bandwidth\"], \"type\": \"string\"}, \"direction\": {\"title\": \"Direction\", \"description\": \"Indicates the direction of target packets. Available vaules include from (the packets from target), to (the packets to target), and both (the packets from or to target). This parameter makes Chaos only take effect for a specific direction of packets.\", \"default\": \"to\", \"example\": \"both\", \"enum\": [\"from\", \"to\", \"both\"], \"type\": \"string\"}, \"target\": {\"title\": \"Target\", \"description\": \"Used in combination with direction, making Chaos only effective for some packets. 'from' and 'both' direction cannot be used when targets is empty in netem action.\", \"allOf\": [{\"$ref\": \"#/definitions/Selector\"}]}, \"mode\": {\"title\": \"Mode\", \"description\": \"Specifies the mode of the experiment. The mode options include one (selecting a random Pod), all (selecting all eligible Pods), fixed (selecting a specified number of eligible Pods), fixed-percent (selecting a specified percentage of Pods from the eligible Pods), and random-max-percent (selecting the maximum percentage of Pods from the eligible Pods)\", \"example\": \"one\", \"enum\": [\"one\", \"all\", \"fixed\", \"fixed-percent\", \"random-max-percent\"], \"type\": \"string\"}, \"value\": {\"title\": \"Value\", \"description\": \"Provides parameters for the mode configuration, depending on mode. For example, when mode is set to fixed-percent, value specifies the percentage of Pods.\", \"example\": \"1\", \"type\": \"string\"}, \"selector\": {\"title\": \"Selector\", \"description\": \"Specifies the target Pod.\", \"allOf\": [{\"$ref\": \"#/definitions/Selectors\"}]}, \"externalTargets\": {\"title\": \"Externaltargets\", \"description\": \"Indicates the network targets except for Kubernetes, which can be IPv4 addresses or domains. This parameter only works with direction: to.\", \"example\": [\"1.1.1.1\", \"www.google.com\"], \"type\": \"array\", \"items\": {\"type\": \"string\"}}, \"device\": {\"title\": \"Device\", \"description\": \"Specifies the affected network interface\", \"example\": \"eth0\", \"type\": \"string\"}, \"delay\": {\"title\": \"Delay\", \"description\": \"When setting action to delay means simulating network delay fault, you also need to configure this parameters.\", \"allOf\": [{\"$ref\": \"#/definitions/Deplay\"}]}, \"loss\": {\"title\": \"Loss\", \"description\": \"When setting action to loss means simulating packet loss fault, you can also configure this parameters.\", \"allOf\": [{\"$ref\": \"#/definitions/Loss\"}]}, \"duplicated\": {\"title\": \"Duplicated\", \"description\": \"When setting action to duplicate, meaning simulating package duplication, you can also set this parameters.\", \"allOf\": [{\"$ref\": \"#/definitions/Duplicate\"}]}, \"corrupt\": {\"title\": \"Corrupt\", \"description\": \"When setting action to corrupt means simulating package corruption fault, you can also configure the following parameters.\", \"allOf\": [{\"$ref\": \"#/definitions/Corrupt\"}]}, \"rate\": {\"title\": \"Rate\", \"description\": \"When setting action to rate means simulating bandwidth rate fault, you also need to configure this parameters. This action is similar to bandwidth/rate below, however, the key distinction is that this action can combine with other netem actions listed above. However, if you require more control over the bandwidth simulation such as limiting the buffer size, select the bandwidth action.\", \"allOf\": [{\"$ref\": \"#/definitions/Rate\"}]}, \"bandwidth\": {\"title\": \"Bandwidth\", \"description\": \"When setting 'action' to 'bandwidth' means simulating bandwidth limit fault, you also need to configure this parameters. This action is mutually exclusive with any netem action defined above. If you need to inject bandwidth rate along with other network failures such as corruption, use the rate action instead.\", \"allOf\": [{\"$ref\": \"#/definitions/Bandwidth\"}]}}, \"required\": [\"action\", \"mode\", \"selector\"], \"definitions\": {\"SetBasedRequirements\": {\"title\": \"SetBasedRequirements\", \"type\": \"object\", \"properties\": {\"key\": {\"title\": \"Key\", \"description\": \"Label key\", \"type\": \"string\"}, \"operator\": {\"title\": \"Operator\", \"description\": \"Select an operator.\", \"enum\": [\"In\", \"NotIn\", \"Exists\", \"DoesNotExist\"], \"type\": \"string\"}, \"values\": {\"title\": \"Values\", \"description\": \"Label values. The values set must be non-empty in the case of In and NotIn.\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"key\", \"operator\", \"values\"]}, \"Selectors\": {\"title\": \"Selectors\", \"type\": \"object\", \"properties\": {\"namespaces\": {\"title\": \"Namespaces\", \"description\": \"Specifies the namespace of the experiment's target Pod. If this selector is None, Chaos Mesh will set it to the namespace of the current Chaos experiment.\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}, \"labelSelectors\": {\"title\": \"Labelselectors\", \"description\": \"Specifies the label-key/value pairs that the experiment's target Pod must have. If multiple labels are specified, the experiment target must have all the labels specified by this selector.\", \"type\": \"object\", \"additionalProperties\": {\"type\": \"string\"}}, \"expressionSelectors\": {\"title\": \"Expressionselectors\", \"description\": \"Specifies a set of expressions that define the label's rules to specifiy the experiment's target Pod.\", \"example\": [{\"key\": \"tier\", \"operator\": \"In\", \"values\": [\"cache\"]}, {\"key\": \"environment\", \"operator\": \"NotIn\", \"values\": [\"dev\"]}], \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/SetBasedRequirements\"}}, \"annotationSelectors\": {\"title\": \"Annotationselectors\", \"description\": \"Specifies the annotation-key/value pairs that the experiment's target Pod must have. If multiple annotations are specified, the experiment target must have all annotations specified by this selector.\", \"type\": \"object\", \"additionalProperties\": {\"type\": \"string\"}}, \"fieldSelectors\": {\"title\": \"Fieldselectors\", \"description\": \"Specifies the field-key/value pairs of the experiment's target Pod. If multiple fields are specified, the experiment target must have all fields set by this selector.\", \"example\": {\"metadata.name\": \"my-pod\", \"metadata.namespace\": \"dafault\"}, \"type\": \"object\", \"additionalProperties\": {\"type\": \"string\"}}, \"podPhaseSelectors\": {\"title\": \"Podphaseselectors\", \"description\": \"Specifies the phase of the experiment's target Pod. If this selector is None, the target Pod's phase is not limited.\", \"type\": \"array\", \"items\": {\"enum\": [\"Pending\", \"Running\", \"Succeeded\", \"Failed\", \"Unknown\"], \"type\": \"string\"}}, \"nodeSelectors\": {\"title\": \"Nodeselectors\", \"description\": \"Specifies the node-label-key/value pairs to which the experiment's target Pod belongs.\", \"type\": \"object\", \"additionalProperties\": {\"type\": \"string\"}}, \"nodes\": {\"title\": \"Nodes\", \"description\": \"Specifies the node to which the experiment's target Pod belongs. The target Pod can only belong to one node in the configured node list. If multiple node labels are specified, the node to which the experiment's target Pod belongs must have all labels specified by this selector.\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}, \"pods\": {\"title\": \"Pods\", \"description\": \"Specifies the namespaces and list of the experiment's target Pods. If you have specified this selector, Chaos Mesh ignores other configured selectors.\", \"example\": {\"default\": [\"pod-0\", \"pod-2\"]}, \"type\": \"object\", \"additionalProperties\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}}}}, \"Selector\": {\"title\": \"Selector\", \"type\": \"object\", \"properties\": {\"mode\": {\"title\": \"Mode\", \"description\": \"Specifies the mode of the experiment. The mode options include one (selecting a random Pod), all (selecting all eligible Pods), fixed (selecting a specified number of eligible Pods), fixed-percent (selecting a specified percentage of Pods from the eligible Pods), and random-max-percent (selecting the maximum percentage of Pods from the eligible Pods)\", \"example\": \"one\", \"enum\": [\"one\", \"all\", \"fixed\", \"fixed-percent\", \"random-max-percent\"], \"type\": \"string\"}, \"selector\": {\"title\": \"Selector\", \"description\": \"Specifies the target Pod.\", \"example\": null, \"allOf\": [{\"$ref\": \"#/definitions/Selectors\"}]}}, \"required\": [\"mode\", \"selector\"]}, \"Reorder\": {\"title\": \"Reorder\", \"type\": \"object\", \"properties\": {\"reorder\": {\"title\": \"Reorder\", \"description\": \"Indicates the probability to reorder\", \"default\": \"0\", \"example\": \"0.5\", \"type\": \"string\"}, \"correlation\": {\"title\": \"Correlation\", \"description\": \"Indicates the correlation between this time's length of delay time and the previous time's length of delay time. Range of value: [0, 100]\", \"default\": \"0\", \"example\": \"50\", \"type\": \"string\"}, \"gap\": {\"title\": \"Gap\", \"description\": \"Indicates the gap before and after packet reordering\", \"default\": 0, \"example\": 5, \"type\": \"integer\"}}}, \"Deplay\": {\"title\": \"Deplay\", \"type\": \"object\", \"properties\": {\"latency\": {\"title\": \"Latency\", \"description\": \"Indicates the network latency\", \"example\": \"2ms\", \"type\": \"string\"}, \"correlation\": {\"title\": \"Correlation\", \"description\": \"Indicates the correlation between the current latency and the previous one. Range of value: [0, 100]. Specify only the number. NEVER include any units.\", \"example\": \"50\", \"type\": \"string\"}, \"jitter\": {\"title\": \"Jitter\", \"description\": \"Indicates the range of the network latency\", \"example\": \"1ms\", \"type\": \"string\"}, \"reorder\": {\"title\": \"Reorder\", \"description\": \"Indicates the status of network packet reordering\", \"allOf\": [{\"$ref\": \"#/definitions/Reorder\"}]}}}, \"Loss\": {\"title\": \"Loss\", \"type\": \"object\", \"properties\": {\"loss\": {\"title\": \"Loss\", \"description\": \"Indicates the probability of packet loss. Range of value: [0, 100]. Specify only the number. NEVER include any units.\", \"default\": \"0\", \"example\": \"50\", \"type\": \"string\"}, \"correlation\": {\"title\": \"Correlation\", \"description\": \"Indicates the correlation between the probability of current packet loss and the previous time's packet loss. Range of value: [0, 100]. Specify only the number. NEVER include any units.\", \"default\": \"0\", \"example\": \"50\", \"type\": \"string\"}}}, \"Duplicate\": {\"title\": \"Duplicate\", \"type\": \"object\", \"properties\": {\"duplicate\": {\"title\": \"Duplicate\", \"description\": \"Indicates the probability of packet duplicating. Range of value: [0, 100]. Specify only the number. NEVER include any units.\", \"default\": \"0\", \"example\": \"50\", \"type\": \"string\"}, \"correlation\": {\"title\": \"Correlation\", \"description\": \"Indicates the correlation between the probability of current packet duplicating and the previous time's packet duplicating. Range of value: [0, 100]. Specify only the number. NEVER include any units.\", \"default\": \"0\", \"example\": \"50\", \"type\": \"string\"}}}, \"Corrupt\": {\"title\": \"Corrupt\", \"type\": \"object\", \"properties\": {\"corrupt\": {\"title\": \"Corrupt\", \"description\": \"Indicates the probability of packet corruption. Range of value: [0, 100]. Specify only the number. NEVER include any units.\", \"default\": \"0\", \"example\": \"50\", \"type\": \"string\"}, \"correlation\": {\"title\": \"Correlation\", \"description\": \"Indicates the correlation between the probability of current packet corruption and the previous time's packet corruption. Range of value: [0, 100]. Specify only the number. NEVER include any units.\", \"default\": \"0\", \"example\": \"50\", \"type\": \"string\"}}}, \"Rate\": {\"title\": \"Rate\", \"type\": \"object\", \"properties\": {\"rate\": {\"title\": \"Rate\", \"description\": \"Indicates the rate of bandwidth limit. Allows bit, kbit, mbit, gbit, tbit, bps, kbps, mbps, gbps, tbps unit. bps means bytes per second\", \"example\": \"1mbps\", \"type\": \"string\"}}}, \"Bandwidth\": {\"title\": \"Bandwidth\", \"type\": \"object\", \"properties\": {\"rate\": {\"title\": \"Rate\", \"description\": \"Indicates the rate of bandwidth limit. Allows bit, kbit, mbit, gbit, tbit, bps, kbps, mbps, gbps, tbps unit. bps means bytes per second\", \"example\": \"1mbps\", \"type\": \"string\"}, \"limit\": {\"title\": \"Limit\", \"description\": \"Indicates the number of bytes waiting in queue\", \"example\": 1, \"type\": \"integer\"}, \"buffer\": {\"title\": \"Buffer\", \"description\": \"Indicates the maximum number of bytes that can be sent instantaneously\", \"example\": 1, \"type\": \"integer\"}, \"peakrate\": {\"title\": \"Peakrate\", \"description\": \"Indicates the maximum consumption of bucket (usually not set)\", \"example\": 1, \"type\": \"integer\"}, \"minburst\": {\"title\": \"Minburst\", \"description\": \"Indicates the size of peakrate bucket (usually not set)\", \"example\": 1, \"type\": \"integer\"}}}}}\n```\nAI: ```json\n{\"action\":"
            ],
            "{\"action\": \"partition\", \"mode\": \"one\", \"selector\": {\"namespaces\": [\"default\"], \"labelSelectors\": {\"app\": \"example\"}}, \"direction\": \"both\"}\n"
        ]
    }
]