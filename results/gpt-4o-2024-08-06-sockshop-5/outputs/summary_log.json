[
    {
        "name": "overall_summary",
        "token_usage": {
            "input_tokens": 18687,
            "output_tokens": 368,
            "total_tokens": 19055
        },
        "message_history": [
            [
                "System: You are a helpful AI assistant for Chaos Engineering.\nGiven a summary of a Chaos Engineering cycle, please elaborate the summary.\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n\nHere is the output schema:\n```\n{\"properties\": {\"summary\": {\"title\": \"Summary\", \"type\": \"string\"}}, \"required\": [\"summary\"]}\n```\nHuman: Here is the overview of a Chaos Engineering Cycle:\n# Here is a Chaos Engineering cycle\n## Step 0. User-input understanding\n### Here is the overview of user inputs:\nThe system consists of the following K8s manifest(s):K8s manifest: sock-shop-2/manifests/00-sock-shop-ns.yaml\n```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: sock-shop\n\n```\nSummary of sock-shop-2/manifests/00-sock-shop-ns.yaml:\n- This manifest defines a Kubernetes Namespace.\n- The Namespace is named 'sock-shop'.\n- Namespaces are used to organize and manage resources in a Kubernetes cluster.\n\nK8s manifest: sock-shop-2/manifests/01-carts-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: carts\n  labels:\n    name: carts\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: carts\n  template:\n    metadata:\n      labels:\n        name: carts\n    spec:\n      containers:\n      - name: carts\n        image: weaveworksdemos/carts:0.4.8\n        env:\n         - name: JAVA_OPTS\n           value: -Xms64m -Xmx128m -XX:+UseG1GC -Djava.security.egd=file:/dev/urandom -Dspring.zipkin.enabled=false\n        resources:\n          limits:\n            cpu: 300m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        ports:\n        - containerPort: 80\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 10001\n          capabilities:\n            drop:\n              - all\n            add:\n              - NET_BIND_SERVICE\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n      volumes:\n        - name: tmp-volume\n          emptyDir:\n            medium: Memory\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n\n```\nSummary of sock-shop-2/manifests/01-carts-dep.yaml:\n- This manifest defines a Deployment in Kubernetes.\n- The Deployment is named 'carts' and is located in the 'sock-shop' namespace.\n- It specifies that there should be 2 replicas of the 'carts' application running.\n- The Deployment uses the Docker image 'weaveworksdemos/carts:0.4.8'.\n- Environment variables are set for Java options to optimize memory usage and disable certain features.\n- Resource limits and requests are defined, with a maximum of 300m CPU and 500Mi memory, and a minimum of 100m CPU and 200Mi memory.\n- The application listens on port 80 within the container.\n- Security settings ensure the container runs as a non-root user with specific capabilities and a read-only root filesystem.\n- A temporary volume is mounted at '/tmp' using an in-memory empty directory.\n- The Deployment is scheduled to run on nodes with the Linux operating system.\n\nK8s manifest: sock-shop-2/manifests/02-carts-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: carts\n  annotations:\n        prometheus.io/scrape: 'true'\n  labels:\n    name: carts\n  namespace: sock-shop\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 80\n    targetPort: 80\n  selector:\n    name: carts\n\n```\nSummary of sock-shop-2/manifests/02-carts-svc.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'carts'.\n- It is annotated to enable Prometheus scraping with 'prometheus.io/scrape: true'.\n- The Service is labeled with 'name: carts'.\n- It is deployed in the 'sock-shop' namespace.\n- The Service exposes port 80 and directs traffic to the same port on the selected pods.\n- It uses a selector to target pods with the label 'name: carts'.\n\nK8s manifest: sock-shop-2/manifests/03-carts-db-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: carts-db\n  labels:\n    name: carts-db\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: carts-db\n  template:\n    metadata:\n      labels:\n        name: carts-db\n    spec:\n      containers:\n      - name: carts-db\n        image: mongo\n        ports:\n        - name: mongo\n          containerPort: 27017\n        securityContext:\n          capabilities:\n            drop:\n              - all\n            add:\n              - CHOWN\n              - SETGID\n              - SETUID\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n      volumes:\n        - name: tmp-volume\n          emptyDir:\n            medium: Memory\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n\n```\nSummary of sock-shop-2/manifests/03-carts-db-dep.yaml:\n- This manifest defines a Deployment in Kubernetes.\n- The Deployment is named 'carts-db' and is located in the 'sock-shop' namespace.\n- It specifies that there should be 2 replicas of the 'carts-db' pod running.\n- The pods are selected based on the label 'name: carts-db'.\n- Each pod runs a single container using the 'mongo' image.\n- The container exposes port 27017, which is the default port for MongoDB.\n- Security settings are applied to drop all capabilities and only add CHOWN, SETGID, and SETUID.\n- The root filesystem of the container is set to read-only for security purposes.\n- A temporary volume is mounted at '/tmp' using an in-memory emptyDir volume.\n- The pods are scheduled to run on nodes with the operating system labeled as 'linux'.\n\nK8s manifest: sock-shop-2/manifests/04-carts-db-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: carts-db\n  labels:\n    name: carts-db\n  namespace: sock-shop\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 27017\n    targetPort: 27017\n  selector:\n    name: carts-db\n\n```\nSummary of sock-shop-2/manifests/04-carts-db-svc.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'carts-db'.\n- It is labeled with 'name: carts-db'.\n- The Service is created in the 'sock-shop' namespace.\n- It exposes port 27017 and directs traffic to the same port on the target pods.\n- The Service selects pods with the label 'name: carts-db' to route traffic to them.\n\nK8s manifest: sock-shop-2/manifests/05-catalogue-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalogue\n  labels:\n    name: catalogue\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: catalogue\n  template:\n    metadata:\n      labels:\n        name: catalogue\n    spec:\n      containers:\n      - name: catalogue\n        image: weaveworksdemos/catalogue:0.3.5\n        command: [\"/app\"]\n        args:\n        - -port=80\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 80\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 10001\n          capabilities:\n            drop:\n              - all\n            add:\n              - NET_BIND_SERVICE\n          readOnlyRootFilesystem: true\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 300\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 180\n          periodSeconds: 3\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n\n```\nSummary of sock-shop-2/manifests/05-catalogue-dep.yaml:\n- This manifest defines a Deployment in Kubernetes.\n- The Deployment is named 'catalogue' and is part of the 'sock-shop' namespace.\n- It specifies that there should be 2 replicas of the 'catalogue' application running.\n- The Deployment uses the Docker image 'weaveworksdemos/catalogue:0.3.5'.\n- The application runs with the command '/app' and listens on port 80.\n- Resource limits are set to 200m CPU and 200Mi memory, with requests for 100m CPU and 100Mi memory.\n- The container is configured to run as a non-root user with user ID 10001.\n- Security settings include dropping all capabilities except 'NET_BIND_SERVICE' and using a read-only root filesystem.\n- Liveness and readiness probes are configured to check the '/health' endpoint on port 80, with initial delays of 300 and 180 seconds respectively.\n- The Deployment is scheduled to run on nodes with the Linux operating system.\n\nK8s manifest: sock-shop-2/manifests/06-catalogue-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: catalogue\n  annotations:\n        prometheus.io/scrape: 'true'\n  labels:\n    name: catalogue\n  namespace: sock-shop\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 80\n    targetPort: 80\n  selector:\n    name: catalogue\n\n```\nSummary of sock-shop-2/manifests/06-catalogue-svc.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'catalogue'.\n- It is annotated to enable Prometheus scraping for monitoring purposes.\n- The Service is labeled with 'name: catalogue'.\n- It is deployed in the 'sock-shop' namespace.\n- The Service exposes port 80 and directs traffic to the same port on the selected pods.\n- It uses a selector to target pods with the label 'name: catalogue'.\n\nK8s manifest: sock-shop-2/manifests/07-catalogue-db-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalogue-db\n  labels:\n    name: catalogue-db\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: catalogue-db\n  template:\n    metadata:\n      labels:\n        name: catalogue-db\n    spec:\n      containers:\n      - name: catalogue-db\n        image: weaveworksdemos/catalogue-db:0.3.0\n        env:\n          - name: MYSQL_ROOT_PASSWORD\n            value: fake_password\n          - name: MYSQL_DATABASE\n            value: socksdb\n        ports:\n        - name: mysql\n          containerPort: 3306\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n\n```\nSummary of sock-shop-2/manifests/07-catalogue-db-dep.yaml:\n- This manifest defines a Deployment in Kubernetes.\n- The Deployment is named 'catalogue-db'.\n- It is located in the 'sock-shop' namespace.\n- The Deployment will create 2 replicas of the pod.\n- Each pod will run a container from the image 'weaveworksdemos/catalogue-db:0.3.0'.\n- The container is configured with environment variables for MySQL, including a root password and database name.\n- The container exposes port 3306, which is commonly used for MySQL.\n- The pods are scheduled to run on nodes with the Linux operating system.\n\nK8s manifest: sock-shop-2/manifests/08-catalogue-db-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: catalogue-db\n  labels:\n    name: catalogue-db\n  namespace: sock-shop\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 3306\n    targetPort: 3306\n  selector:\n    name: catalogue-db\n\n```\nSummary of sock-shop-2/manifests/08-catalogue-db-svc.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'catalogue-db'.\n- It is associated with the 'sock-shop' namespace.\n- The Service listens on port 3306 and forwards traffic to the same port on the target pods.\n- It uses a selector to target pods with the label 'name: catalogue-db'.\n\nK8s manifest: sock-shop-2/manifests/09-front-end-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: front-end\n  namespace: sock-shop\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: front-end\n  template:\n    metadata:\n      labels:\n        name: front-end\n    spec:\n      containers:\n      - name: front-end\n        image: weaveworksdemos/front-end:0.3.12\n        resources:\n          limits:\n            cpu: 300m\n            memory: 1000Mi\n          requests:\n            cpu: 100m\n            memory: 300Mi\n        ports:\n        - containerPort: 8079\n        env:\n        - name: SESSION_REDIS\n          value: \"true\"\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 10001\n          capabilities:\n            drop:\n              - all\n          readOnlyRootFilesystem: true\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8079\n          initialDelaySeconds: 300\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8079\n          initialDelaySeconds: 30\n          periodSeconds: 3\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n```\nSummary of sock-shop-2/manifests/09-front-end-dep.yaml:\n- This manifest defines a Deployment in Kubernetes.\n- The Deployment is named 'front-end' and is located in the 'sock-shop' namespace.\n- It specifies that there should be 1 replica of the front-end application running.\n- The Deployment uses a selector to match pods with the label 'name: front-end'.\n- The pod template includes a single container named 'front-end'.\n- The container uses the image 'weaveworksdemos/front-end:0.3.12'.\n- Resource limits are set for the container: 300m CPU and 1000Mi memory.\n- Resource requests are set for the container: 100m CPU and 300Mi memory.\n- The container exposes port 8079.\n- An environment variable 'SESSION_REDIS' is set to 'true'.\n- Security context is configured to run the container as a non-root user with user ID 10001.\n- All Linux capabilities are dropped, and the root filesystem is set to read-only.\n- A liveness probe is configured to check the '/' path on port 8079, with an initial delay of 300 seconds and a period of 3 seconds.\n- A readiness probe is also configured to check the '/' path on port 8079, with an initial delay of 30 seconds and a period of 3 seconds.\n- The node selector ensures that the pod runs on nodes with the operating system labeled as Linux.\n\nK8s manifest: sock-shop-2/manifests/10-front-end-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: front-end\n  annotations:\n        prometheus.io/scrape: 'true'\n  labels:\n    name: front-end\n  namespace: sock-shop\nspec:\n  type: NodePort\n  ports:\n  - port: 80\n    targetPort: 8079\n    nodePort: 30001\n  selector:\n    name: front-end\n\n```\nSummary of sock-shop-2/manifests/10-front-end-svc.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'front-end'.\n- It is located in the 'sock-shop' namespace.\n- The Service type is 'NodePort', which exposes the service on each Node's IP at a static port.\n- It listens on port 80 and forwards traffic to target port 8079 on the pods.\n- The nodePort is set to 30001, allowing external access to the service.\n- The Service is configured to be scraped by Prometheus for monitoring, as indicated by the annotation 'prometheus.io/scrape: true'.\n- It selects pods with the label 'name: front-end' to route traffic to.\n\nK8s manifest: sock-shop-2/manifests/11-orders-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: orders\n  labels:\n    name: orders\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: orders\n  template:\n    metadata:\n      labels:\n        name: orders\n    spec:\n      containers:\n      - name: orders\n        image: weaveworksdemos/orders:0.4.7\n        env:\n         - name: JAVA_OPTS\n           value: -Xms64m -Xmx128m -XX:+UseG1GC -Djava.security.egd=file:/dev/urandom -Dspring.zipkin.enabled=false\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 300Mi\n        ports:\n        - containerPort: 80\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 10001\n          capabilities:\n            drop:\n              - all\n            add:\n              - NET_BIND_SERVICE\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n      volumes:\n        - name: tmp-volume\n          emptyDir:\n            medium: Memory\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n\n```\nSummary of sock-shop-2/manifests/11-orders-dep.yaml:\n- This manifest defines a Deployment in Kubernetes.\n- The Deployment is named 'orders' and is located in the 'sock-shop' namespace.\n- It specifies that there should be 2 replicas of the 'orders' application running.\n- The Deployment uses the 'weaveworksdemos/orders:0.4.7' Docker image for the container.\n- Environment variables are set for Java options to optimize memory usage and disable certain features.\n- Resource limits and requests are defined, with a maximum of 500m CPU and 500Mi memory, and a minimum of 100m CPU and 300Mi memory.\n- The container listens on port 80.\n- Security context is configured to run the container as a non-root user with specific capabilities and a read-only root filesystem.\n- A temporary volume is mounted at '/tmp' using an in-memory empty directory.\n- The Deployment is scheduled to run on nodes with the Linux operating system.\n\nK8s manifest: sock-shop-2/manifests/12-orders-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: orders\n  annotations:\n        prometheus.io/scrape: 'true'\n  labels:\n    name: orders\n  namespace: sock-shop\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 80\n    targetPort: 80\n  selector:\n    name: orders\n\n```\nSummary of sock-shop-2/manifests/12-orders-svc.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'orders'.\n- It is annotated to enable Prometheus scraping with 'prometheus.io/scrape: true'.\n- The Service is labeled with 'name: orders'.\n- It is deployed in the 'sock-shop' namespace.\n- The Service exposes port 80 and directs traffic to the same port on the target pods.\n- It uses a selector to match pods with the label 'name: orders'.\n\nK8s manifest: sock-shop-2/manifests/13-orders-db-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: orders-db\n  labels:\n    name: orders-db\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: orders-db\n  template:\n    metadata:\n      labels:\n        name: orders-db\n    spec:\n      containers:\n      - name: orders-db\n        image: mongo\n        ports:\n        - name: mongo\n          containerPort: 27017\n        securityContext:\n          capabilities:\n            drop:\n              - all\n            add:\n              - CHOWN\n              - SETGID\n              - SETUID\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n      volumes:\n        - name: tmp-volume\n          emptyDir:\n            medium: Memory\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n\n```\nSummary of sock-shop-2/manifests/13-orders-db-dep.yaml:\n- This manifest defines a Deployment in Kubernetes.\n- The Deployment is named 'orders-db' and is located in the 'sock-shop' namespace.\n- It specifies 2 replicas of the 'orders-db' pod to be created.\n- The pods are labeled with 'name: orders-db' for identification and selection.\n- Each pod runs a single container using the 'mongo' image.\n- The container exposes port 27017, which is the default port for MongoDB.\n- Security settings are applied to drop all capabilities and add only CHOWN, SETGID, and SETUID.\n- The root filesystem of the container is set to read-only for security purposes.\n- A temporary volume is mounted at '/tmp' using an in-memory emptyDir volume.\n- The pods are scheduled to run on nodes with the operating system labeled as 'linux'.\n\nK8s manifest: sock-shop-2/manifests/14-orders-db-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: orders-db\n  labels:\n    name: orders-db\n  namespace: sock-shop\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 27017\n    targetPort: 27017\n  selector:\n    name: orders-db\n\n```\nSummary of sock-shop-2/manifests/14-orders-db-svc.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'orders-db'.\n- It is located in the 'sock-shop' namespace.\n- The Service is configured to expose port 27017.\n- It targets the same port (27017) on the pods it selects.\n- The Service uses a selector to match pods with the label 'name: orders-db'.\n\nK8s manifest: sock-shop-2/manifests/15-payment-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: payment\n  labels:\n    name: payment\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: payment\n  template:\n    metadata:\n      labels:\n        name: payment\n    spec:\n      containers:\n      - name: payment\n        image: weaveworksdemos/payment:0.4.3\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 99m\n            memory: 100Mi\n        ports:\n        - containerPort: 80\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 10001\n          capabilities:\n            drop:\n              - all\n            add:\n              - NET_BIND_SERVICE\n          readOnlyRootFilesystem: true\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 300\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 180\n          periodSeconds: 3\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n\n```\nSummary of sock-shop-2/manifests/15-payment-dep.yaml:\n- This manifest defines a Deployment in Kubernetes.\n- The Deployment is named 'payment' and is part of the 'sock-shop' namespace.\n- It specifies that there should be 2 replicas of the 'payment' application running.\n- The Deployment uses the Docker image 'weaveworksdemos/payment:0.4.3'.\n- Resource limits are set for the container, with a maximum of 200m CPU and 200Mi memory, and requests for 99m CPU and 100Mi memory.\n- The container listens on port 80.\n- Security settings ensure the container runs as a non-root user with user ID 10001, drops all capabilities except 'NET_BIND_SERVICE', and uses a read-only root filesystem.\n- Liveness and readiness probes are configured to check the '/health' endpoint on port 80, with initial delays of 300 and 180 seconds respectively, and a period of 3 seconds.\n- The Deployment is scheduled to run on nodes with the Linux operating system.\n\nK8s manifest: sock-shop-2/manifests/16-payment-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: payment\n  annotations:\n        prometheus.io/scrape: 'true'\n  labels:\n    name: payment\n  namespace: sock-shop\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 80\n    targetPort: 80\n  selector:\n    name: payment\n\n```\nSummary of sock-shop-2/manifests/16-payment-svc.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'payment'.\n- It is annotated for Prometheus scraping, which means it is set up for monitoring.\n- The Service is labeled with 'name: payment'.\n- It is deployed in the 'sock-shop' namespace.\n- The Service exposes port 80 and directs traffic to the same port on the selected pods.\n- The Service selects pods with the label 'name: payment'.\n\nK8s manifest: sock-shop-2/manifests/17-queue-master-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: queue-master\n  labels:\n    name: queue-master\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: queue-master\n  template:\n    metadata:\n      labels:\n        name: queue-master\n    spec:\n      containers:\n      - name: queue-master\n        image: weaveworksdemos/queue-master:0.3.1\n        env:\n         - name: JAVA_OPTS\n           value: -Xms64m -Xmx128m -XX:+UseG1GC -Djava.security.egd=file:/dev/urandom -Dspring.zipkin.enabled=false\n        resources:\n          limits:\n            cpu: 300m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 300Mi\n        ports:\n        - containerPort: 80\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n\n```\nSummary of sock-shop-2/manifests/17-queue-master-dep.yaml:\n- This manifest defines a Deployment in Kubernetes.\n- The Deployment is named 'queue-master' and is located in the 'sock-shop' namespace.\n- It specifies that there should be 2 replicas (instances) of the 'queue-master' application running.\n- The Deployment uses a container image 'weaveworksdemos/queue-master:0.3.1'.\n- Environment variables are set for the container, including Java options for memory management and garbage collection.\n- Resource limits and requests are defined, with a CPU limit of 300m and memory limit of 500Mi, and requests for 100m CPU and 300Mi memory.\n- The container exposes port 80 for network traffic.\n- The Deployment is configured to run on nodes with the Linux operating system.\n\nK8s manifest: sock-shop-2/manifests/18-queue-master-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: queue-master\n  annotations:\n        prometheus.io/scrape: 'true'\n  labels:\n    name: queue-master\n  namespace: sock-shop\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 80\n    targetPort: 80\n  selector:\n    name: queue-master\n```\nSummary of sock-shop-2/manifests/18-queue-master-svc.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'queue-master'.\n- It is annotated for Prometheus scraping with 'prometheus.io/scrape: true'.\n- The Service is labeled with 'name: queue-master'.\n- It is deployed in the 'sock-shop' namespace.\n- The Service exposes port 80 and directs traffic to the same port on the selected pods.\n- It selects pods with the label 'name: queue-master' to route traffic to.\n\nK8s manifest: sock-shop-2/manifests/19-rabbitmq-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rabbitmq\n  labels:\n    name: rabbitmq\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: rabbitmq\n  template:\n    metadata:\n      labels:\n        name: rabbitmq\n      annotations:\n        prometheus.io/scrape: \"false\"\n    spec:\n      containers:\n      - name: rabbitmq\n        image: rabbitmq:3.6.8-management\n        ports:\n        - containerPort: 15672\n          name: management\n        - containerPort: 5672\n          name: rabbitmq\n        securityContext:\n          capabilities:\n            drop:\n              - all\n            add:\n              - CHOWN\n              - SETGID\n              - SETUID\n              - DAC_OVERRIDE\n          readOnlyRootFilesystem: true\n      - name: rabbitmq-exporter\n        image: kbudde/rabbitmq-exporter\n        ports:\n        - containerPort: 9090\n          name: exporter\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n\n```\nSummary of sock-shop-2/manifests/19-rabbitmq-dep.yaml:\n- This manifest defines a Deployment for RabbitMQ in Kubernetes.\n- It is set to run in the 'sock-shop' namespace.\n- The Deployment is named 'rabbitmq' and is labeled accordingly.\n- It specifies 2 replicas, meaning there will be 2 instances of RabbitMQ running.\n- The Deployment uses a selector to match pods with the label 'name: rabbitmq'.\n- The pod template includes two containers: one for RabbitMQ and another for a RabbitMQ exporter.\n- The RabbitMQ container uses the image 'rabbitmq:3.6.8-management'.\n- It exposes two ports: 15672 for management and 5672 for RabbitMQ operations.\n- Security context is set to drop all capabilities and add specific ones like CHOWN, SETGID, SETUID, and DAC_OVERRIDE.\n- The root filesystem is set to read-only for security purposes.\n- The RabbitMQ exporter container uses the image 'kbudde/rabbitmq-exporter' and exposes port 9090.\n- The Deployment is configured to run on nodes with the label 'beta.kubernetes.io/os: linux'.\n- Annotations are set to prevent Prometheus from scraping metrics from this deployment.\n\nK8s manifest: sock-shop-2/manifests/20-rabbitmq-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: rabbitmq\n  annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n  labels:\n    name: rabbitmq\n  namespace: sock-shop\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 5672\n    name: rabbitmq\n    targetPort: 5672\n  - port: 9090\n    name: exporter\n    targetPort: exporter\n    protocol: TCP\n  selector:\n    name: rabbitmq\n\n```\nSummary of sock-shop-2/manifests/20-rabbitmq-svc.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'rabbitmq'.\n- It is annotated for Prometheus scraping on port 9090.\n- The Service is labeled with 'name: rabbitmq'.\n- It is deployed in the 'sock-shop' namespace.\n- The Service exposes two ports: 5672 for RabbitMQ and 9090 for an exporter.\n- The protocol used for the ports is TCP.\n- The Service selects pods with the label 'name: rabbitmq'.\n\nK8s manifest: sock-shop-2/manifests/21-session-db-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: session-db\n  labels:\n    name: session-db\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: session-db\n  template:\n    metadata:\n      labels:\n        name: session-db\n      annotations:\n        prometheus.io.scrape: \"false\"\n    spec:\n      containers:\n      - name: session-db\n        image: redis:alpine\n        ports:\n        - name: redis\n          containerPort: 6379\n        securityContext:\n          capabilities:\n            drop:\n              - all\n            add:\n              - CHOWN\n              - SETGID\n              - SETUID\n          readOnlyRootFilesystem: true\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n\n```\nSummary of sock-shop-2/manifests/21-session-db-dep.yaml:\n- This manifest defines a Deployment in Kubernetes.\n- The Deployment is named 'session-db' and is located in the 'sock-shop' namespace.\n- It specifies that there should be 2 replicas of the 'session-db' pod running.\n- The pods are selected based on the label 'name: session-db'.\n- Each pod runs a single container using the 'redis:alpine' image.\n- The container exposes port 6379, which is commonly used by Redis.\n- Security settings are applied to drop all capabilities and only add CHOWN, SETGID, and SETUID, with a read-only root filesystem for enhanced security.\n- The pods are scheduled to run on nodes with the operating system labeled as Linux.\n\nK8s manifest: sock-shop-2/manifests/22-session-db-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: session-db\n  labels:\n    name: session-db\n  namespace: sock-shop\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 6379\n    targetPort: 6379\n  selector:\n    name: session-db\n\n```\nSummary of sock-shop-2/manifests/22-session-db-svc.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'session-db'.\n- It is located in the 'sock-shop' namespace.\n- The Service is configured to expose port 6379.\n- It targets the same port (6379) on the selected pods.\n- The Service uses a selector to match pods with the label 'name: session-db'.\n\nK8s manifest: sock-shop-2/manifests/23-shipping-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: shipping\n  labels:\n    name: shipping\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: shipping\n  template:\n    metadata:\n      labels:\n        name: shipping\n    spec:\n      containers:\n      - name: shipping\n        image: weaveworksdemos/shipping:0.4.8\n        env:\n         - name: ZIPKIN\n           value: zipkin.jaeger.svc.cluster.local\n         - name: JAVA_OPTS\n           value: -Xms64m -Xmx128m -XX:+UseG1GC -Djava.security.egd=file:/dev/urandom -Dspring.zipkin.enabled=false\n        resources:\n          limits:\n            cpu: 300m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 300Mi\n        ports:\n        - containerPort: 80\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 10001\n          capabilities:\n            drop:\n              - all\n            add:\n              - NET_BIND_SERVICE\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n      volumes:\n        - name: tmp-volume\n          emptyDir:\n            medium: Memory\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n\n```\nSummary of sock-shop-2/manifests/23-shipping-dep.yaml:\n- This manifest defines a Deployment in Kubernetes.\n- The Deployment is named 'shipping' and is located in the 'sock-shop' namespace.\n- It specifies that there should be 2 replicas of the 'shipping' application running.\n- The Deployment uses the Docker image 'weaveworksdemos/shipping:0.4.8'.\n- Environment variables are set for the application, including 'ZIPKIN' and 'JAVA_OPTS'.\n- Resource limits and requests are defined, with limits set to 300m CPU and 500Mi memory, and requests set to 100m CPU and 300Mi memory.\n- The application listens on port 80.\n- Security context is configured to run the container as a non-root user with user ID 10001, and it drops all capabilities except 'NET_BIND_SERVICE'.\n- The root filesystem is set to be read-only.\n- A temporary volume is mounted at '/tmp', using an in-memory emptyDir volume.\n- The Deployment is scheduled to run on nodes with the operating system labeled as 'linux'.\n\nK8s manifest: sock-shop-2/manifests/24-shipping-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: shipping\n  annotations:\n        prometheus.io/scrape: 'true'\n  labels:\n    name: shipping\n  namespace: sock-shop\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 80\n    targetPort: 80\n  selector:\n    name: shipping\n\n\n```\nSummary of sock-shop-2/manifests/24-shipping-svc.yaml:\n- This is a Kubernetes Service manifest.\n- The service is named 'shipping'.\n- It is annotated for Prometheus scraping with 'prometheus.io/scrape: true'.\n- The service is labeled with 'name: shipping'.\n- It is deployed in the 'sock-shop' namespace.\n- The service exposes port 80 and directs traffic to the same port on the selected pods.\n- It selects pods with the label 'name: shipping' to route traffic to.\n\nK8s manifest: sock-shop-2/manifests/25-user-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user\n  labels:\n    name: user\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: user\n  template:\n    metadata:\n      labels:\n        name: user\n    spec:\n      containers:\n      - name: user\n        image: weaveworksdemos/user:0.4.7\n        resources:\n          limits:\n            cpu: 300m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 80\n        env:\n        - name: mongo\n          value: user-db:27017\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 10001\n          capabilities:\n            drop:\n              - all\n            add:\n              - NET_BIND_SERVICE\n          readOnlyRootFilesystem: true\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 300\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 180\n          periodSeconds: 3\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n\n```\nSummary of sock-shop-2/manifests/25-user-dep.yaml:\n- This manifest defines a Deployment in Kubernetes.\n- The Deployment is named 'user' and is located in the 'sock-shop' namespace.\n- It specifies that there should be 2 replicas of the 'user' application running.\n- The Deployment uses the Docker image 'weaveworksdemos/user:0.4.7'.\n- Resource limits are set for the container: 300m CPU and 200Mi memory, with requests for 100m CPU and 100Mi memory.\n- The container listens on port 80.\n- An environment variable 'mongo' is set with the value 'user-db:27017'.\n- Security context is configured to run the container as a non-root user with user ID 10001.\n- The container has a read-only root filesystem and drops all capabilities except 'NET_BIND_SERVICE'.\n- Liveness and readiness probes are configured to check the '/health' endpoint on port 80.\n- The liveness probe starts after 300 seconds and checks every 3 seconds.\n- The readiness probe starts after 180 seconds and checks every 3 seconds.\n- The Deployment is scheduled to run on nodes with the Linux operating system.\n\nK8s manifest: sock-shop-2/manifests/26-user-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: user\n  annotations:\n        prometheus.io/scrape: 'true'\n  labels:\n    name: user\n  namespace: sock-shop\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 80\n    targetPort: 80\n  selector:\n    name: user\n\n\n```\nSummary of sock-shop-2/manifests/26-user-svc.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'user'.\n- It is annotated for Prometheus scraping, which means it is set up for monitoring.\n- The Service is labeled with 'name: user'.\n- It is deployed in the 'sock-shop' namespace.\n- The Service listens on port 80 and forwards traffic to the same port on the selected pods.\n- The Service selects pods with the label 'name: user'.\n\nK8s manifest: sock-shop-2/manifests/27-user-db-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-db\n  labels:\n    name: user-db\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: user-db\n  template:\n    metadata:\n      labels:\n        name: user-db\n    spec:\n      containers:\n      - name: user-db\n        image: weaveworksdemos/user-db:0.3.0\n\n        ports:\n        - name: mongo\n          containerPort: 27017\n        securityContext:\n          capabilities:\n            drop:\n              - all\n            add:\n              - CHOWN\n              - SETGID\n              - SETUID\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n      volumes:\n        - name: tmp-volume\n          emptyDir:\n            medium: Memory\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n\n```\nSummary of sock-shop-2/manifests/27-user-db-dep.yaml:\n- This manifest defines a Deployment in Kubernetes.\n- The Deployment is named 'user-db' and is located in the 'sock-shop' namespace.\n- It specifies that there should be 2 replicas of the 'user-db' pod running.\n- The pods are selected based on the label 'name: user-db'.\n- Each pod runs a single container using the image 'weaveworksdemos/user-db:0.3.0'.\n- The container exposes port 27017, labeled as 'mongo', which is typically used for MongoDB.\n- Security settings are applied to drop all capabilities and only add CHOWN, SETGID, and SETUID, with a read-only root filesystem.\n- A temporary volume is mounted at '/tmp' using an in-memory emptyDir volume.\n- The pods are scheduled to run on nodes with the operating system labeled as 'linux'.\n\nK8s manifest: sock-shop-2/manifests/28-user-db-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: user-db\n  labels:\n    name: user-db\n  namespace: sock-shop\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 27017\n    targetPort: 27017\n  selector:\n    name: user-db\n\n\n```\nSummary of sock-shop-2/manifests/28-user-db-svc.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'user-db'.\n- It is labeled with 'name: user-db'.\n- The Service is created in the 'sock-shop' namespace.\n- It exposes port 27017 and directs traffic to the same port on the target pods.\n- The Service selects pods with the label 'name: user-db' to route traffic to them.\n\nThe resiliency issues/weaknesses in the system are as follows:\nIssue #0: Missing Resource Requests\n  - details: Pods may not get scheduled if the cluster is under resource pressure, leading to potential downtime.\n  - manifests having the issues: ['sock-shop-2/manifests/03-carts-db-dep.yaml', 'sock-shop-2/manifests/07-catalogue-db-dep.yaml', 'sock-shop-2/manifests/13-orders-db-dep.yaml', 'sock-shop-2/manifests/19-rabbitmq-dep.yaml', 'sock-shop-2/manifests/21-session-db-dep.yaml', 'sock-shop-2/manifests/27-user-db-dep.yaml']\n  - problematic config: The deployments for carts-db, catalogue-db, orders-db, rabbitmq, session-db, and user-db do not specify resource requests.\n\nIssue #1: Single Replica Deployment\n  - details: The front-end deployment has only one replica, which can lead to downtime if the pod fails.\n  - manifests having the issues: ['sock-shop-2/manifests/09-front-end-dep.yaml']\n  - problematic config: spec.replicas: 1\n\nIssue #2: Missing Liveness and Readiness Probes\n  - details: Without liveness and readiness probes, Kubernetes cannot determine the health of the containers, which may lead to serving traffic to unhealthy pods.\n  - manifests having the issues: ['sock-shop-2/manifests/01-carts-dep.yaml', 'sock-shop-2/manifests/03-carts-db-dep.yaml', 'sock-shop-2/manifests/05-catalogue-dep.yaml', 'sock-shop-2/manifests/07-catalogue-db-dep.yaml', 'sock-shop-2/manifests/11-orders-dep.yaml', 'sock-shop-2/manifests/13-orders-db-dep.yaml', 'sock-shop-2/manifests/15-payment-dep.yaml', 'sock-shop-2/manifests/17-queue-master-dep.yaml', 'sock-shop-2/manifests/19-rabbitmq-dep.yaml', 'sock-shop-2/manifests/21-session-db-dep.yaml', 'sock-shop-2/manifests/23-shipping-dep.yaml', 'sock-shop-2/manifests/25-user-dep.yaml', 'sock-shop-2/manifests/27-user-db-dep.yaml']\n  - problematic config: The deployments for carts, carts-db, catalogue, catalogue-db, orders, orders-db, payment, queue-master, rabbitmq, session-db, shipping, user, and user-db do not have liveness or readiness probes configured.\n\nIssue #3: Hardcoded Environment Variables\n  - details: Hardcoded environment variables, such as passwords, can lead to security vulnerabilities.\n  - manifests having the issues: ['sock-shop-2/manifests/07-catalogue-db-dep.yaml']\n  - problematic config: env: MYSQL_ROOT_PASSWORD is hardcoded with value: fake_password.\n\nThe expected type of application on the system (i.e., K8s manfests):\nSock Shop is an e-commerce application that simulates an online sock store. It offers users the ability to browse a catalogue of socks, add items to a cart, place orders, and manage user accounts. The application is composed of multiple microservices, each responsible for a specific function, such as managing the product catalogue, handling user sessions, processing payments, and managing orders.; The manifests provided are part of a well-known microservices demo application called 'Sock Shop'. This application is designed to demonstrate the use of microservices architecture and Kubernetes orchestration. The application simulates an e-commerce website for selling socks, with various services such as carts, catalogue, orders, payment, and user management. The file names, labels, and Docker images used in the manifests (e.g., 'weaveworksdemos') are indicative of the Sock Shop application, which is a common reference application in the Kubernetes community.\n\nChaos-Engineering instructions for the system are as follows: - The Chaos-Engineering experiment must be completed within 1 minute.\n- When using k6 in steady-state definition, always select a request URL from the following options (other requests are invalid):\n  1. http://front-end.sock-shop.svc.cluster.local/\n  2. http://front-end.sock-shop.svc.cluster.local/catalogue?size=10\n  3. http://front-end.sock-shop.svc.cluster.local/detail.html?id=<ID>\n  Replace <ID> with an available ID: [`03fef6ac-1896-4ce8-bd69-b798f85c6e0b`, `3395a43e-2d88-40de-b95f-e00e1502085b`, `510a0d7e-8e83-4193-b483-e27e09ddc34d`, `808a2de1-1aaa-4c25-a9b9-6612e8f29a38`, `819e1fbf-8b7e-4f6d-811f-693534916a8b`, `837ab141-399e-4c1f-9abc-bace40296bac`, `a0a4f044-b040-410d-8ead-4de0446aec7e`, `d3588630-ad8e-49df-bbd7-3167f7efb246`, `zzz4f044-b040-410d-8ead-4de0446aec7e`]\n  4. http://front-end.sock-shop.svc.cluster.local/category/\n  5. http://front-end.sock-shop.svc.cluster.local/category?tags=<TAG>\n  Replace <TAG> with an available tag: [`magic`, `action`, `blue`, `brown`, `black`, `sport`, `formal`, `red`, `green`, `skin`, `geek`]\n  6. http://front-end.sock-shop.svc.cluster.local/basket.html\n\n## Step 1. Hypothesis definition\n### Here is the overview of the hypothesis for the system:\nThe hypothesis is \"The steady states of the sytem are maintained even when the fault scenario occurs (i.e., when the faults are injected)\".\nThe steady states here are as follows:\n2 steady states are defined.\n1st steady states:\n- Name: carts-db-replicas\n- Description: The first issue to address is the 'Missing Resource Requests' for the 'carts-db' deployment. This is a critical issue because without resource requests, the pods may not get scheduled if the cluster is under resource pressure, leading to potential downtime. To ensure the system's steady state, we need to verify that the 'carts-db' deployment maintains the expected number of replicas, which is 2, even under resource constraints. This can be measured by checking the number of running pods for the 'carts-db' deployment.\n- Threshold for the steady state: At least 1 ready replica 100% of the time and 2 ready replicas at least 80% of the time during the monitoring period.; The steady state we are considering is the number of replicas for the 'carts-db' deployment. The current state shows that there are 2 total replicas and 2 ready replicas consistently over a period of 5 seconds. This indicates that the system is currently stable with all replicas being ready. Given that the deployment is configured to have 2 replicas, the threshold should ensure that both replicas are ready to maintain the steady state. To account for some fluctuations, we can set a tolerance that allows for a brief period where not all replicas are ready, but the system should recover quickly. A reasonable threshold would be to have at least 1 ready replica 100% of the time and both replicas ready at least 80% of the time during the monitoring period. This ensures that the system remains operational even if one replica temporarily becomes unavailable.\n- Whether the steady state meets the threshold is determined by the following Python script with K8s API:\n```\nimport os\nimport time\nimport argparse\nfrom kubernetes import client, config\nfrom unittest_base import K8sAPIBase\n\nclass TestCartsDBReplicas(K8sAPIBase):\n    def __init__(self, namespace='sock-shop', deployment_name='carts-db', duration=5):\n        super().__init__()\n        self.namespace = namespace\n        self.deployment_name = deployment_name\n        self.duration = duration\n        # Use the correct API client for deployments\n        self.apps_v1 = client.AppsV1Api()\n\n    def test_replicas_threshold(self):\n        # Initialize counters for ready replicas\n        total_checks = 0\n        ready_replicas_count = 0\n        fully_ready_replicas_count = 0\n\n        for _ in range(self.duration):\n            try:\n                # Read the deployment status using the correct API client\n                deployment = self.apps_v1.read_namespaced_deployment(self.deployment_name, self.namespace)\n                replicas = deployment.status.replicas\n                ready_replicas = deployment.status.ready_replicas\n                print(f\"Total replicas: {replicas}, Ready replicas: {ready_replicas}\")\n\n                # Increment the total checks\n                total_checks += 1\n\n                # Check if at least 1 replica is ready\n                if ready_replicas >= 1:\n                    ready_replicas_count += 1\n\n                # Check if both replicas are ready\n                if ready_replicas == 2:\n                    fully_ready_replicas_count += 1\n\n            except client.exceptions.ApiException as e:\n                print(f\"Exception when calling AppsV1Api->read_namespaced_deployment: {e}\")\n\n            # Wait for 1 second before the next check\n            time.sleep(1)\n\n        # Calculate the percentage of time conditions are met\n        one_ready_percentage = (ready_replicas_count / total_checks) * 100\n        two_ready_percentage = (fully_ready_replicas_count / total_checks) * 100\n\n        # Assert the threshold conditions\n        assert one_ready_percentage == 100, \"At least 1 ready replica was not available 100% of the time.\"\n        assert two_ready_percentage >= 80, \"2 ready replicas were not available at least 80% of the time.\"\n\n        print(\"Test passed: Steady state conditions are satisfied.\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='Test carts-db replicas threshold')\n    parser.add_argument('--duration', type=int, default=5, help='Duration to check the replicas')\n    args = parser.parse_args()\n\n    # Create an instance of the test class with the specified duration\n    test = TestCartsDBReplicas(duration=args.duration)\n    # Run the test\n    test.test_replicas_threshold()\n\n\nif __name__ == '__main__':\n    main()\n\n```2nd steady states:\n- Name: front-end-replica\n- Description: The next issue to address is the 'Single Replica Deployment' for the 'front-end' deployment. This is a critical issue because having only one replica can lead to downtime if the pod fails. To ensure the system's steady state, we need to verify that the 'front-end' deployment maintains at least one running pod at all times. This can be measured by checking the number of running pods for the 'front-end' deployment. Given that the deployment is configured to have only 1 replica, the threshold should ensure that this single replica is always ready to maintain the steady state. A reasonable threshold would be to have at least 1 ready replica 100% of the time during the monitoring period. This ensures that the system remains operational even if there are temporary issues with the pod.\n- Threshold for the steady state: At least 1 ready replica must be present 100% of the time during the monitoring period.; The steady state we are considering is the availability of the 'front-end' deployment, which currently has only one replica. The current state shows that the 'front-end' deployment has 1 ready replica consistently over a monitoring period of 5 seconds. To ensure the system remains operational, the threshold should be set such that the 'front-end' deployment maintains at least 1 ready replica 100% of the time during the monitoring period. This accounts for the fact that there is only one replica, and any downtime would mean the service is unavailable. Given the current state satisfies this condition, the threshold should be set to ensure that at least 1 ready replica is present 100% of the time during the monitoring period.\n- Whether the steady state meets the threshold is determined by the following Python script with K8s API:\n```\nimport os\nimport time\nimport argparse\nfrom kubernetes import client, config\nfrom unittest_base import K8sAPIBase\n\nclass TestFrontEndReplica(K8sAPIBase):\n    def __init__(self, namespace, deployment_name, duration):\n        super().__init__()\n        self.namespace = namespace\n        self.deployment_name = deployment_name\n        self.duration = duration\n        # Use AppsV1Api to interact with deployments\n        self.apps_v1 = client.AppsV1Api()\n\n    def test_steady_state(self):\n        ready_replicas_count = 0\n\n        # Loop for the specified duration\n        for _ in range(self.duration):\n            # Get the deployment status\n            resp = self.apps_v1.read_namespaced_deployment_status(self.deployment_name, self.namespace)\n            ready_replicas = resp.status.ready_replicas or 0\n            print(f\"Ready replicas for {self.deployment_name}: {ready_replicas}\")\n\n            # Check if the number of ready replicas is at least 1\n            if ready_replicas >= 1:\n                ready_replicas_count += 1\n\n            # Wait for 1 second before the next check\n            time.sleep(1)\n\n        # Calculate the percentage of time the deployment was ready\n        readiness_percentage = (ready_replicas_count / self.duration) * 100\n        print(f\"{self.deployment_name} was ready {ready_replicas_count}/{self.duration} times.\")\n\n        # Assert that the deployment was ready 100% of the time\n        assert readiness_percentage == 100, f\"{self.deployment_name} readiness was {readiness_percentage}%, expected 100%.\"\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='Test front-end replica readiness')\n    parser.add_argument('--duration', type=int, default=5, help='Duration to check the replicas')\n    args = parser.parse_args()\n\n    # Create a test instance and run the test\n    test = TestFrontEndReplica('sock-shop', 'front-end', args.duration)\n    test.test_steady_state()\n\n\nif __name__ == '__main__':\n    main()\n```\n\nThe fault scenario here is as follows:\n\nAn assumed fault scenario is as follows:\n- Event: Black Friday Sale\n- Used Chaos Engineering tool: Chaos Mesh\n- Faults to simulate the event: [[Fault(name='StressChaos', name_id=0, params={'mode': 'all', 'selector': {'namespaces': ['sock-shop'], 'labelSelectors': {'name': 'carts-db'}}, 'stressors': {'cpu': {'workers': 2, 'load': 80}}, 'containerNames': ['carts-db']})], [Fault(name='PodChaos', name_id=0, params={'action': 'pod-kill', 'mode': 'one', 'selector': {'namespaces': ['sock-shop'], 'labelSelectors': {'name': 'front-end'}}, 'value': '1'})]]\n- Description: During a Black Friday sale, the system is expected to handle a significant increase in traffic, which can stress the system's resources and reveal weaknesses. The 'carts-db' deployment lacks resource requests, which can lead to scheduling issues under resource pressure. The 'front-end' deployment has only one replica, making it a single point of failure. To simulate the Black Friday event, we will first inject a 'StressChaos' fault to simulate high CPU usage on the 'carts-db' pods, testing if the system can maintain the steady state of having at least 1 ready replica 100% of the time and 2 ready replicas 80% of the time. Next, we will inject a 'PodChaos' fault to kill the 'front-end' pod, testing if the system can maintain the steady state of having at least 1 ready replica 100% of the time. This sequence simulates the increased load and potential failures during a high-traffic event like Black Friday.\n\n## Step 2.1. Chaos-Engineering experiment\n### Here is the overview of my Chaos-Engineering experiment to verify the hypothesis:\nThe entire time schedule of the Chaos-Engineering experiment is as follows (The experiment is divided into three phases: pre-validation, fault-injection, and post-validation phases):\nThe chaos engineering experiment is designed to test the system's resilience under stress conditions and potential failures. Given the constraints, the experiment must be completed within 1 minute. The experiment is divided into three phases: pre-validation, fault-injection, and post-validation. \n\n1. **Pre-validation Phase**: This phase ensures that the system is in a steady state before any faults are injected. We will allocate 20 seconds for this phase. During this time, we will verify that the 'carts-db' deployment maintains at least 1 ready replica 100% of the time and 2 ready replicas 80% of the time, and that the 'front-end' deployment maintains at least 1 ready replica 100% of the time.\n\n2. **Fault-injection Phase**: This phase involves injecting faults to simulate the Black Friday event. We will allocate 20 seconds for this phase. The faults include a 'StressChaos' fault to simulate high CPU usage on the 'carts-db' pods and a 'PodChaos' fault to kill the 'front-end' pod. These faults will test the system's ability to maintain the defined steady states under stress and failure conditions.\n\n3. **Post-validation Phase**: This phase ensures that the system returns to its steady state after the faults are removed. We will allocate 20 seconds for this phase. During this time, we will again verify the steady states for the 'carts-db' and 'front-end' deployments to ensure they meet the defined thresholds.\n\nThe total time for the experiment is 60 seconds, with each phase receiving an equal allocation of 20 seconds. This allocation allows for a balanced approach to validating the system's steady state, injecting faults, and confirming recovery.\n- Total experiment phase: 60s\n- Pre-validation phase: 20s\n- Fault-injection phase: 20s\n- Post-validation phase: 20s\n\nThe details of the three phases are as follows:\nPre-validation Phase (20s):\nIn the pre-validation phase, we need to ensure that the system is in a steady state before we introduce any faults. This involves verifying that the current state of the system meets the defined steady state thresholds. Given the constraints of a 20-second total time for this phase, we will execute the unit tests for both steady states simultaneously to maximize efficiency. The first steady state, 'carts-db-replicas', requires checking that at least 1 replica is ready 100% of the time and 2 replicas are ready at least 80% of the time. The second steady state, 'front-end-replica', requires ensuring that the single replica is ready 100% of the time. Both tests will run for the full 20 seconds, with checks occurring every second. This approach ensures that we have sufficient data to confirm the system's readiness before proceeding to the fault injection phase.\n\nFault-injection Phase (20s):\nIn this fault-injection phase, we aim to simulate a Black Friday sale scenario by introducing two types of faults: 'StressChaos' and 'PodChaos'. The goal is to observe how the system behaves under stress and potential failure conditions. The phase is limited to 20 seconds, so we need to carefully time the injections and tests to fit within this window.\n\nFirst, we will inject a 'StressChaos' fault on the 'carts-db' deployment to simulate high CPU usage. This will help us test the system's ability to maintain the steady state of having at least 1 ready replica 100% of the time and 2 ready replicas 80% of the time. We will start this fault injection immediately at the beginning of the phase and let it run for 10 seconds.\n\nSimultaneously, we will run the unit test for the 'carts-db-replicas' steady state to verify if the system can maintain the required number of ready replicas during the stress period. This test will also start at the beginning of the phase and run for 10 seconds.\n\nNext, we will inject a 'PodChaos' fault to kill the 'front-end' pod. This will test the system's ability to maintain the steady state of having at least 1 ready replica 100% of the time for the 'front-end' deployment. We will start this fault injection at the 10-second mark and let it run for 5 seconds.\n\nFinally, we will run the unit test for the 'front-end-replica' steady state to verify if the system can maintain the required readiness during the pod failure. This test will start at the 10-second mark and run for 5 seconds.\n\nBy staggering the fault injections and unit tests in this manner, we can effectively simulate the Black Friday scenario and assess the system's resilience under stress and failure conditions.\n\nPost-validation Phase (20s):\nIn the post-validation phase, we need to ensure that the system has returned to its steady states after the fault injection. The steady states we are concerned with are the 'carts-db-replicas' and 'front-end-replica'. The post-validation phase will involve running unit tests to verify these steady states. Given the total time of 20 seconds for this phase, we will stagger the unit tests to ensure both steady states are verified within this timeframe. \n\nThe 'carts-db-replicas' steady state requires that at least 1 ready replica is available 100% of the time and 2 ready replicas are available at least 80% of the time. We will allocate 10 seconds for this test, with a grace period of 0 seconds, as we want to immediately verify the recovery of the 'carts-db' deployment after the stress test.\n\nThe 'front-end-replica' steady state requires that at least 1 ready replica is available 100% of the time. We will allocate the remaining 10 seconds for this test, also with a grace period of 0 seconds, to ensure the 'front-end' deployment has recovered from the pod kill fault.\n\nBy staggering the tests in this manner, we ensure that both steady states are verified within the 20-second post-validation phase, allowing us to confirm that the system has returned to its expected operational state.\n\nThe summary of the above experiment plan:\nThe chaos engineering experiment is structured into three phases, each lasting 20 seconds, to test the system's resilience. In the pre-validation phase, both unit tests for verifying steady states are executed simultaneously for the entire 20 seconds. The 'carts-db-replicas' test checks that at least one replica is ready 100% of the time and two replicas are ready 80% of the time, while the 'front-end-replica' test ensures one replica is ready 100% of the time. In the fault-injection phase, the 'StressChaos' fault is injected on the 'carts-db' deployment to simulate high CPU usage, starting immediately and lasting for 10 seconds. Concurrently, the 'carts-db-replicas' unit test runs for 10 seconds to verify readiness under stress. At the 10-second mark, the 'PodChaos' fault is injected to kill the 'front-end' pod, lasting for 5 seconds, while the 'front-end-replica' unit test runs for 5 seconds to check readiness during the fault. In the post-validation phase, the 'carts-db-replicas' unit test runs for the first 10 seconds to verify recovery, followed by the 'front-end-replica' unit test for the next 10 seconds, ensuring both deployments return to their steady states.\n\nTo automatically conduct the above experiment plan with Chaos Mesh, the following Chaos-Mesh-Worfklow file was created (by applying it to the cluster, the experiment plan will be automatically executed according to the Chaos-Mesh-Worfklow file):\n```yaml\napiVersion: chaos-mesh.org/v1alpha1\nkind: Workflow\nmetadata:\n  name: chaos-experiment-20241127-045539\nspec:\n  entry: the-entry\n  templates:\n    #-------------------------------\n    # entry point of whole workflow\n    #-------------------------------\n    - name: the-entry\n      templateType: Serial\n      deadline: 30m45s\n      children:\n        - pre-validation-phase\n        - fault-injection-phase\n        - post-validation-phase\n\n    #-------------------------------------\n    # Entry point of pre-validation-phase\n    #-------------------------------------\n    - name: pre-validation-phase\n      templateType: Serial\n      deadline: 10m20s\n      children:\n        - pre-validation-parallel-workflows\n    \n    - name: pre-validation-parallel-workflows\n      templateType: Parallel\n      deadline: 5m20s\n      children:\n        - pre-unittest-carts-db-replicas\n        - pre-unittest-front-end-replica\n\n    # Definitions of children of pre-validation-phase\n    - name: pre-unittest-carts-db-replicas\n      templateType: Task\n      deadline: 5m20s\n      task:\n        container:\n          name: pre-unittest-carts-db-replicas-container\n          image: chaos-eater/k8sapi:1.0\n          imagePullPolicy: IfNotPresent\n          command: [\"/bin/bash\", \"-c\"]\n          args: [\"python /chaos-eater/sandbox/cycle_20241127_043136/unittest_carts-db-replicas_mod0.py --duration 20\"]\n          volumeMounts:\n            - name: pvc-volume\n              mountPath: /chaos-eater\n        volumes:\n          - name: pvc-volume\n            persistentVolumeClaim:\n              claimName: pvc\n\n    - name: pre-unittest-front-end-replica\n      templateType: Task\n      deadline: 5m20s\n      task:\n        container:\n          name: pre-unittest-front-end-replica-container\n          image: chaos-eater/k8sapi:1.0\n          imagePullPolicy: IfNotPresent\n          command: [\"/bin/bash\", \"-c\"]\n          args: [\"python /chaos-eater/sandbox/cycle_20241127_043136/unittest_front-end-replica_mod0.py --duration 20\"]\n          volumeMounts:\n            - name: pvc-volume\n              mountPath: /chaos-eater\n        volumes:\n          - name: pvc-volume\n            persistentVolumeClaim:\n              claimName: pvc\n    \n    #--------------------------------------\n    # Entry point of fault-injection-phase\n    #--------------------------------------\n    - name: fault-injection-phase\n      templateType: Serial\n      deadline: 10m15s\n      children:\n        - fault-injection-overlapped-workflows\n\n    - name: fault-injection-parallel-workflow\n      templateType: Parallel\n      deadline: 5m10s\n      children:\n        - fault-unittest-carts-db-replicas\n        - fault-stresschaos\n\n    - name: fault-injection-suspend-workflow\n      templateType: Serial\n      deadline: 5m15s\n      children:\n        - fault-injection-suspend\n        - fault-injection-parallel-workflows\n\n    - name: fault-injection-suspend\n      templateType: Suspend\n      deadline: 10s\n\n    - name: fault-injection-parallel-workflows\n      templateType: Parallel\n      deadline: 5m5s\n      children:\n        - fault-unittest-front-end-replica\n        - fault-podchaos\n\n    - name: fault-injection-overlapped-workflows\n      templateType: Parallel\n      deadline: 5m15s\n      children:\n        - fault-injection-parallel-workflow\n        - fault-injection-suspend-workflow\n\n    # Definitions of children of pre-validation-phase\n    # unit tests\n    - name: fault-unittest-carts-db-replicas\n      templateType: Task\n      deadline: 5m10s\n      task:\n        container:\n          name: fault-unittest-carts-db-replicas-container\n          image: chaos-eater/k8sapi:1.0\n          imagePullPolicy: IfNotPresent\n          command: [\"/bin/bash\", \"-c\"]\n          args: [\"python /chaos-eater/sandbox/cycle_20241127_043136/unittest_carts-db-replicas_mod0.py --duration 10\"]\n          volumeMounts:\n            - name: pvc-volume\n              mountPath: /chaos-eater\n        volumes:\n          - name: pvc-volume\n            persistentVolumeClaim:\n              claimName: pvc\n\n    - name: fault-unittest-front-end-replica\n      templateType: Task\n      deadline: 5m5s\n      task:\n        container:\n          name: fault-unittest-front-end-replica-container\n          image: chaos-eater/k8sapi:1.0\n          imagePullPolicy: IfNotPresent\n          command: [\"/bin/bash\", \"-c\"]\n          args: [\"python /chaos-eater/sandbox/cycle_20241127_043136/unittest_front-end-replica_mod0.py --duration 5\"]\n          volumeMounts:\n            - name: pvc-volume\n              mountPath: /chaos-eater\n        volumes:\n          - name: pvc-volume\n            persistentVolumeClaim:\n              claimName: pvc\n\n    # fault_injections\n    - name: fault-stresschaos\n      templateType: StressChaos\n      deadline: 10s\n      stressChaos:\n        containerNames:\n          - carts-db\n        mode: all\n        selector:\n          labelSelectors:\n            name: carts-db\n          namespaces:\n            - sock-shop\n        stressors:\n          cpu:\n            load: 80\n            workers: 2\n\n\n    - name: fault-podchaos\n      templateType: PodChaos\n      deadline: 5s\n      podChaos:\n        action: pod-kill\n        mode: one\n        selector:\n          labelSelectors:\n            name: front-end\n          namespaces:\n            - sock-shop\n        value: '1'\n\n    \n    #--------------------------------------\n    # Entry point of post-validation phase\n    #--------------------------------------\n    - name: post-validation-phase\n      templateType: Serial\n      deadline: 10m10s\n      children:\n        - post-validation-parallel-workflows\n\n    - name: post-validation-parallel-workflows\n      templateType: Parallel\n      deadline: 5m10s\n      children:\n        - post-unittest-carts-db-replicas\n        - post-unittest-front-end-replica\n\n    # Definitions of children of pre-validation-phase\n    - name: post-unittest-carts-db-replicas\n      templateType: Task\n      deadline: 5m10s\n      task:\n        container:\n          name: post-unittest-carts-db-replicas-container\n          image: chaos-eater/k8sapi:1.0\n          imagePullPolicy: IfNotPresent\n          command: [\"/bin/bash\", \"-c\"]\n          args: [\"python /chaos-eater/sandbox/cycle_20241127_043136/unittest_carts-db-replicas_mod0.py --duration 10\"]\n          volumeMounts:\n            - name: pvc-volume\n              mountPath: /chaos-eater\n        volumes:\n          - name: pvc-volume\n            persistentVolumeClaim:\n              claimName: pvc\n\n    - name: post-unittest-front-end-replica\n      templateType: Task\n      deadline: 5m10s\n      task:\n        container:\n          name: post-unittest-front-end-replica-container\n          image: chaos-eater/k8sapi:1.0\n          imagePullPolicy: IfNotPresent\n          command: [\"/bin/bash\", \"-c\"]\n          args: [\"python /chaos-eater/sandbox/cycle_20241127_043136/unittest_front-end-replica_mod0.py --duration 10\"]\n          volumeMounts:\n            - name: pvc-volume\n              mountPath: /chaos-eater\n        volumes:\n          - name: pvc-volume\n            persistentVolumeClaim:\n              claimName: pvc\n```\n\n## Step 2.2, 3, 4. Experiment execution, analysis and improvement (reconfiguring the system to satisfy the hypothesis)\n### Here is the improvement history:\n### Experiment result (1st try)\nPassed unittests:\n- pre-unittest-carts-db-replicas\n- pre-unittest-front-end-replica\n- fault-unittest-carts-db-replicas\n- post-unittest-carts-db-replicas\nFailed unittests:\n- fault-unittest-front-end-replica\n```log\nTraceback (most recent call last):\n  File \"/chaos-eater/sandbox/cycle_20241127_043136/hypothesis/unittest_front-end-replica_mod1.py\", line 52, in <module>\n    main()\n  File \"/chaos-eater/sandbox/cycle_20241127_043136/hypothesis/unittest_front-end-replica_mod1.py\", line 48, in main\n    test.test_steady_state()\n  File \"/chaos-eater/sandbox/cycle_20241127_043136/hypothesis/unittest_front-end-replica_mod1.py\", line 38, in test_steady_state\n    assert readiness_percentage == 100, f\"{self.deployment_name} readiness was {readiness_percentage}%, expected 100%.\"\nAssertionError: front-end readiness was 0.0%, expected 100%.\nReady replicas for front-end: 0\nReady replicas for front-end: 0\nReady replicas for front-end: 0\nReady replicas for front-end: 0\nReady replicas for front-end: 0\nfront-end was ready 0/5 times.\n\n```\n\n- post-unittest-front-end-replica\n```log\nReady replicas for front-end: 0\nReady replicas for front-end: 0\nReady replicas for front-end: 0\nReady replicas for front-end: 0\nReady replicas for front-end: 0\nTraceback (most recent call last):\n  File \"/chaos-eater/sandbox/cycle_20241127_043136/hypothesis/unittest_front-end-replica_mod1.py\", line 52, in <module>\nReady replicas for front-end: 0\nReady replicas for front-end: 0\nReady replicas for front-end: 0\nReady replicas for front-end: 0\nReady replicas for front-end: 0\nfront-end was ready 0/10 times.\n    main()\n  File \"/chaos-eater/sandbox/cycle_20241127_043136/hypothesis/unittest_front-end-replica_mod1.py\", line 48, in main\n    test.test_steady_state()\n  File \"/chaos-eater/sandbox/cycle_20241127_043136/hypothesis/unittest_front-end-replica_mod1.py\", line 38, in test_steady_state\n    assert readiness_percentage == 100, f\"{self.deployment_name} readiness was {readiness_percentage}%, expected 100%.\"\nAssertionError: front-end readiness was 0.0%, expected 100%.\n\n```\n\n\n\n### Analysis report (1st try)\nThe chaos engineering experiment aimed to test the resilience of the 'sock-shop' system under simulated fault conditions, specifically focusing on the 'carts-db' and 'front-end' deployments. The experiment was structured into three phases: pre-validation, fault-injection, and post-validation. The results indicate that while the 'carts-db' deployment maintained its steady state throughout the experiment, the 'front-end' deployment failed to meet the expected steady state during both the fault-injection and post-validation phases.\n\n1. **Pre-Validation Phase:**\n   - Both the 'carts-db-replicas' and 'front-end-replica' tests passed, indicating that the system was in a stable state before any faults were injected. This confirms that the initial configuration of the system was functioning as expected.\n\n2. **Fault-Injected Phase:**\n   - The 'carts-db-replicas' test passed during the fault-injection phase, demonstrating that the 'carts-db' deployment was resilient to the 'StressChaos' fault, which simulated high CPU usage. This suggests that despite the lack of resource requests, the 'carts-db' deployment was able to maintain at least one ready replica 100% of the time and two ready replicas 80% of the time.\n   - The 'front-end-replica' test failed during the fault-injection phase. The 'PodChaos' fault, which involved killing the 'front-end' pod, resulted in 0 ready replicas throughout the test duration. This failure highlights the critical issue of having only a single replica for the 'front-end' deployment, making it a single point of failure. The system was unable to recover or maintain availability when the single pod was terminated.\n\n3. **Post-Validation Phase:**\n   - The 'carts-db-replicas' test continued to pass in the post-validation phase, indicating that the 'carts-db' deployment returned to its steady state after the fault was removed.\n   - The 'front-end-replica' test failed again in the post-validation phase, with 0 ready replicas recorded throughout the test. This suggests that the 'front-end' deployment did not recover after the fault was removed, likely due to the absence of additional replicas to take over when the single pod was killed.\n\n**Insights and Recommendations:**\n- The failure of the 'front-end' deployment to maintain its steady state during and after the fault-injection phase underscores the importance of addressing the 'Single Replica Deployment' issue. To improve resilience, it is recommended to increase the number of replicas for the 'front-end' deployment. This will ensure that the system can maintain availability even if one pod fails.\n- Consider implementing horizontal pod autoscaling for the 'front-end' deployment to dynamically adjust the number of replicas based on traffic and resource usage. This can help the system handle sudden spikes in demand, such as those experienced during events like Black Friday.\n- Review and potentially adjust the readiness and liveness probes for the 'front-end' deployment to ensure that Kubernetes can accurately determine the health of the pods and take corrective actions if necessary.\n\nBy addressing these issues, the system's resilience can be significantly improved, reducing the risk of downtime during high-traffic events and fault scenarios.\n\n### Improvement result (1st try)\n1 K8s manifests are modified:\n- The K8s manifest 'sock-shop-2/manifests/09-front-end-dep.yaml' was replaced.\nIncrease the number of replicas from 1 to 2 to improve resiliency and ensure availability during pod failures.\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: front-end\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: front-end\n  template:\n    metadata:\n      labels:\n        name: front-end\n    spec:\n      containers:\n      - name: front-end\n        image: weaveworksdemos/front-end:0.3.12\n        resources:\n          limits:\n            cpu: 300m\n            memory: 1000Mi\n          requests:\n            cpu: 100m\n            memory: 300Mi\n        ports:\n        - containerPort: 8079\n        env:\n        - name: SESSION_REDIS\n          value: \"true\"\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 10001\n          capabilities:\n            drop:\n              - all\n          readOnlyRootFilesystem: true\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8079\n          initialDelaySeconds: 300\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8079\n          initialDelaySeconds: 30\n          periodSeconds: 3\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n```\n\n\n\n### Experiment result (2nd try)\nPassed unittests:\n- pre-unittest-carts-db-replicas\n- pre-unittest-front-end-replica\n- fault-unittest-carts-db-replicas\n- fault-unittest-front-end-replica\n- post-unittest-carts-db-replicas\n- post-unittest-front-end-replica\nFailed unittests:\n\n\nPlease elaborate the above summary of a Chaos Engineering Cycle.\nAI: ```json\n{\"summary\":"
            ],
            "{\"summary\":\"The Chaos Engineering cycle for the 'sock-shop' system involved a detailed analysis and testing of the system's resilience under simulated fault conditions. The system is composed of multiple Kubernetes manifests that define various microservices, such as 'carts', 'catalogue', 'orders', 'payment', and 'user', each with associated deployments and services. The primary focus of the experiment was to address identified resiliency issues, including missing resource requests, single replica deployments, missing liveness and readiness probes, and hardcoded environment variables.\\n\\nThe experiment was structured into three phases: pre-validation, fault-injection, and post-validation, each lasting 20 seconds, to fit within a total duration of 60 seconds. The pre-validation phase ensured that the system was in a steady state before any faults were injected. The fault-injection phase simulated a Black Friday sale scenario by introducing 'StressChaos' and 'PodChaos' faults to test the system's behavior under stress and potential failure conditions. The post-validation phase verified that the system returned to its steady state after the faults were removed.\\n\\nThe initial experiment revealed that while the 'carts-db' deployment maintained its steady state throughout, the 'front-end' deployment failed during the fault-injection and post-validation phases due to having only a single replica, which made it a single point of failure. This led to a recommendation to increase the number of replicas for the 'front-end' deployment to improve resilience.\\n\\nAfter modifying the 'front-end' deployment to have two replicas, a second experiment was conducted. This time, all unit tests passed, indicating that the system successfully maintained its steady states during and after the fault-injection phase. The improvements ensured that the system could handle the simulated high-traffic event and recover from faults, demonstrating enhanced resilience and availability.\"}\n"
        ]
    }
]