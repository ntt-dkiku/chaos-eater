[
    {
        "name": "analysis_experiment",
        "token_usage": {
            "input_tokens": 14125,
            "output_tokens": 679,
            "total_tokens": 14804
        },
        "message_history": [
            [
                "System: You are a helpful AI assistant for Chaos Engineering.\nGiven K8s manifests for a network system, its hypothesis, the overview of a Chaos-Engineeering experiment, and the experimental results, you will analyze the experimental results.\nAlways keep the following rules:\n- Analyze step by step why the test(s) failed, based on the system configuraions (manifests) and the flow of the experiment.\n- Specify the cause while mentioning the corresponding system configurations and the corresponding phenamena in the Chaos-Engineering experiment.\n- The anaysis report here will be used for reconfiguring the system later to avoid the failures and improve resiliency. Therefore, make carefully the report rich in insights so that it will be helpful at that time.\n- When providing insights and reconfiguration recommendations, limit them to areas related to the failed test.\n- The output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n\nHere is the output schema:\n```\n{\"properties\": {\"report\": {\"title\": \"Report\", \"description\": \"Analysis of the experiment result.\", \"type\": \"string\"}}, \"required\": [\"report\"]}\n```\nHuman: # Here is the overview of my system:\nThe system consists of the following K8s manifest(s):K8s manifest: sock-shop-2/manifests/00-sock-shop-ns.yaml\n```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: sock-shop\n\n```\nSummary of sock-shop-2/manifests/00-sock-shop-ns.yaml:\n- This manifest defines a Kubernetes Namespace.\n- The Namespace is named 'sock-shop'.\n- Namespaces are used to organize and manage resources in a Kubernetes cluster.\n\nK8s manifest: sock-shop-2/manifests/01-carts-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: carts\n  labels:\n    name: carts\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: carts\n  template:\n    metadata:\n      labels:\n        name: carts\n    spec:\n      containers:\n      - name: carts\n        image: weaveworksdemos/carts:0.4.8\n        env:\n         - name: JAVA_OPTS\n           value: -Xms64m -Xmx128m -XX:+UseG1GC -Djava.security.egd=file:/dev/urandom -Dspring.zipkin.enabled=false\n        resources:\n          limits:\n            cpu: 300m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        ports:\n        - containerPort: 80\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 10001\n          capabilities:\n            drop:\n              - all\n            add:\n              - NET_BIND_SERVICE\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n      volumes:\n        - name: tmp-volume\n          emptyDir:\n            medium: Memory\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n\n```\nSummary of sock-shop-2/manifests/01-carts-dep.yaml:\n- This manifest defines a Deployment in Kubernetes.\n- The Deployment is named 'carts' and is located in the 'sock-shop' namespace.\n- It specifies that there should be 2 replicas of the 'carts' application running.\n- The Deployment uses the Docker image 'weaveworksdemos/carts:0.4.8'.\n- Environment variables are set for Java options to optimize memory usage and disable certain features.\n- Resource limits and requests are defined, with a maximum of 300m CPU and 500Mi memory, and a minimum of 100m CPU and 200Mi memory.\n- The application listens on port 80 within the container.\n- Security settings ensure the container runs as a non-root user with specific capabilities and a read-only root filesystem.\n- A temporary volume is mounted at '/tmp' using an in-memory empty directory.\n- The Deployment is scheduled to run on nodes with the Linux operating system.\n\nK8s manifest: sock-shop-2/manifests/02-carts-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: carts\n  annotations:\n        prometheus.io/scrape: 'true'\n  labels:\n    name: carts\n  namespace: sock-shop\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 80\n    targetPort: 80\n  selector:\n    name: carts\n\n```\nSummary of sock-shop-2/manifests/02-carts-svc.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'carts'.\n- It is annotated to enable Prometheus scraping with 'prometheus.io/scrape: true'.\n- The Service is labeled with 'name: carts'.\n- It is deployed in the 'sock-shop' namespace.\n- The Service exposes port 80 and directs traffic to the same port on the selected pods.\n- It uses a selector to target pods with the label 'name: carts'.\n\nK8s manifest: sock-shop-2/manifests/03-carts-db-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: carts-db\n  labels:\n    name: carts-db\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: carts-db\n  template:\n    metadata:\n      labels:\n        name: carts-db\n    spec:\n      containers:\n      - name: carts-db\n        image: mongo\n        ports:\n        - name: mongo\n          containerPort: 27017\n        securityContext:\n          capabilities:\n            drop:\n              - all\n            add:\n              - CHOWN\n              - SETGID\n              - SETUID\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n      volumes:\n        - name: tmp-volume\n          emptyDir:\n            medium: Memory\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n\n```\nSummary of sock-shop-2/manifests/03-carts-db-dep.yaml:\n- This manifest defines a Deployment in Kubernetes.\n- The Deployment is named 'carts-db' and is located in the 'sock-shop' namespace.\n- It specifies that there should be 2 replicas of the 'carts-db' pod running.\n- The pods are selected based on the label 'name: carts-db'.\n- Each pod runs a single container using the 'mongo' image.\n- The container exposes port 27017, which is the default port for MongoDB.\n- Security settings are applied to drop all capabilities and only add CHOWN, SETGID, and SETUID.\n- The root filesystem of the container is set to read-only for security purposes.\n- A temporary volume is mounted at '/tmp' using an in-memory emptyDir volume.\n- The pods are scheduled to run on nodes with the operating system labeled as 'linux'.\n\nK8s manifest: sock-shop-2/manifests/04-carts-db-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: carts-db\n  labels:\n    name: carts-db\n  namespace: sock-shop\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 27017\n    targetPort: 27017\n  selector:\n    name: carts-db\n\n```\nSummary of sock-shop-2/manifests/04-carts-db-svc.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'carts-db'.\n- It is labeled with 'name: carts-db'.\n- The Service is created in the 'sock-shop' namespace.\n- It exposes port 27017 and directs traffic to the same port on the target pods.\n- The Service selects pods with the label 'name: carts-db' to route traffic to them.\n\nK8s manifest: sock-shop-2/manifests/05-catalogue-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalogue\n  labels:\n    name: catalogue\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: catalogue\n  template:\n    metadata:\n      labels:\n        name: catalogue\n    spec:\n      containers:\n      - name: catalogue\n        image: weaveworksdemos/catalogue:0.3.5\n        command: [\"/app\"]\n        args:\n        - -port=80\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 80\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 10001\n          capabilities:\n            drop:\n              - all\n            add:\n              - NET_BIND_SERVICE\n          readOnlyRootFilesystem: true\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 300\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 180\n          periodSeconds: 3\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n\n```\nSummary of sock-shop-2/manifests/05-catalogue-dep.yaml:\n- This manifest defines a Deployment in Kubernetes.\n- The Deployment is named 'catalogue' and is part of the 'sock-shop' namespace.\n- It specifies that there should be 2 replicas of the 'catalogue' application running.\n- The Deployment uses the Docker image 'weaveworksdemos/catalogue:0.3.5'.\n- The application runs with the command '/app' and listens on port 80.\n- Resource limits are set to 200m CPU and 200Mi memory, with requests for 100m CPU and 100Mi memory.\n- The container is configured to run as a non-root user with user ID 10001.\n- Security settings include dropping all capabilities except 'NET_BIND_SERVICE' and using a read-only root filesystem.\n- Liveness and readiness probes are configured to check the '/health' endpoint on port 80, with initial delays of 300 and 180 seconds respectively.\n- The Deployment is scheduled to run on nodes with the Linux operating system.\n\nK8s manifest: sock-shop-2/manifests/06-catalogue-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: catalogue\n  annotations:\n        prometheus.io/scrape: 'true'\n  labels:\n    name: catalogue\n  namespace: sock-shop\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 80\n    targetPort: 80\n  selector:\n    name: catalogue\n\n```\nSummary of sock-shop-2/manifests/06-catalogue-svc.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'catalogue'.\n- It is annotated to enable Prometheus scraping for monitoring purposes.\n- The Service is labeled with 'name: catalogue'.\n- It is deployed in the 'sock-shop' namespace.\n- The Service exposes port 80 and directs traffic to the same port on the selected pods.\n- It uses a selector to target pods with the label 'name: catalogue'.\n\nK8s manifest: sock-shop-2/manifests/07-catalogue-db-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalogue-db\n  labels:\n    name: catalogue-db\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: catalogue-db\n  template:\n    metadata:\n      labels:\n        name: catalogue-db\n    spec:\n      containers:\n      - name: catalogue-db\n        image: weaveworksdemos/catalogue-db:0.3.0\n        env:\n          - name: MYSQL_ROOT_PASSWORD\n            value: fake_password\n          - name: MYSQL_DATABASE\n            value: socksdb\n        ports:\n        - name: mysql\n          containerPort: 3306\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n\n```\nSummary of sock-shop-2/manifests/07-catalogue-db-dep.yaml:\n- This manifest defines a Deployment in Kubernetes.\n- The Deployment is named 'catalogue-db'.\n- It is located in the 'sock-shop' namespace.\n- The Deployment will create 2 replicas of the pod.\n- Each pod will run a container from the image 'weaveworksdemos/catalogue-db:0.3.0'.\n- The container is configured with environment variables for MySQL, including a root password and database name.\n- The container exposes port 3306, which is commonly used for MySQL.\n- The pods are scheduled to run on nodes with the Linux operating system.\n\nK8s manifest: sock-shop-2/manifests/08-catalogue-db-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: catalogue-db\n  labels:\n    name: catalogue-db\n  namespace: sock-shop\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 3306\n    targetPort: 3306\n  selector:\n    name: catalogue-db\n\n```\nSummary of sock-shop-2/manifests/08-catalogue-db-svc.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'catalogue-db'.\n- It is associated with the 'sock-shop' namespace.\n- The Service listens on port 3306 and forwards traffic to the same port on the target pods.\n- It uses a selector to target pods with the label 'name: catalogue-db'.\n\nK8s manifest: sock-shop-2/manifests/09-front-end-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: front-end\n  namespace: sock-shop\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: front-end\n  template:\n    metadata:\n      labels:\n        name: front-end\n    spec:\n      containers:\n      - name: front-end\n        image: weaveworksdemos/front-end:0.3.12\n        resources:\n          limits:\n            cpu: 300m\n            memory: 1000Mi\n          requests:\n            cpu: 100m\n            memory: 300Mi\n        ports:\n        - containerPort: 8079\n        env:\n        - name: SESSION_REDIS\n          value: \"true\"\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 10001\n          capabilities:\n            drop:\n              - all\n          readOnlyRootFilesystem: true\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8079\n          initialDelaySeconds: 300\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8079\n          initialDelaySeconds: 30\n          periodSeconds: 3\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n```\nSummary of sock-shop-2/manifests/09-front-end-dep.yaml:\n- This manifest defines a Deployment in Kubernetes.\n- The Deployment is named 'front-end' and is located in the 'sock-shop' namespace.\n- It specifies that there should be 1 replica of the front-end application running.\n- The Deployment uses a selector to match pods with the label 'name: front-end'.\n- The pod template includes a single container named 'front-end'.\n- The container uses the image 'weaveworksdemos/front-end:0.3.12'.\n- Resource limits are set for the container: 300m CPU and 1000Mi memory.\n- Resource requests are set for the container: 100m CPU and 300Mi memory.\n- The container exposes port 8079.\n- An environment variable 'SESSION_REDIS' is set to 'true'.\n- Security context is configured to run the container as a non-root user with user ID 10001.\n- All Linux capabilities are dropped, and the root filesystem is set to read-only.\n- A liveness probe is configured to check the '/' path on port 8079, with an initial delay of 300 seconds and a period of 3 seconds.\n- A readiness probe is also configured to check the '/' path on port 8079, with an initial delay of 30 seconds and a period of 3 seconds.\n- The node selector ensures that the pod runs on nodes with the operating system labeled as Linux.\n\nK8s manifest: sock-shop-2/manifests/10-front-end-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: front-end\n  annotations:\n        prometheus.io/scrape: 'true'\n  labels:\n    name: front-end\n  namespace: sock-shop\nspec:\n  type: NodePort\n  ports:\n  - port: 80\n    targetPort: 8079\n    nodePort: 30001\n  selector:\n    name: front-end\n\n```\nSummary of sock-shop-2/manifests/10-front-end-svc.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'front-end'.\n- It is located in the 'sock-shop' namespace.\n- The Service type is 'NodePort', which exposes the service on each Node's IP at a static port.\n- It listens on port 80 and forwards traffic to target port 8079 on the pods.\n- The nodePort is set to 30001, allowing external access to the service.\n- The Service is configured to be scraped by Prometheus for monitoring, as indicated by the annotation 'prometheus.io/scrape: true'.\n- It selects pods with the label 'name: front-end' to route traffic to.\n\nK8s manifest: sock-shop-2/manifests/11-orders-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: orders\n  labels:\n    name: orders\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: orders\n  template:\n    metadata:\n      labels:\n        name: orders\n    spec:\n      containers:\n      - name: orders\n        image: weaveworksdemos/orders:0.4.7\n        env:\n         - name: JAVA_OPTS\n           value: -Xms64m -Xmx128m -XX:+UseG1GC -Djava.security.egd=file:/dev/urandom -Dspring.zipkin.enabled=false\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 300Mi\n        ports:\n        - containerPort: 80\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 10001\n          capabilities:\n            drop:\n              - all\n            add:\n              - NET_BIND_SERVICE\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n      volumes:\n        - name: tmp-volume\n          emptyDir:\n            medium: Memory\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n\n```\nSummary of sock-shop-2/manifests/11-orders-dep.yaml:\n- This manifest defines a Deployment in Kubernetes.\n- The Deployment is named 'orders' and is located in the 'sock-shop' namespace.\n- It specifies that there should be 2 replicas of the 'orders' application running.\n- The Deployment uses the 'weaveworksdemos/orders:0.4.7' Docker image for the container.\n- Environment variables are set for Java options to optimize memory usage and disable certain features.\n- Resource limits and requests are defined, with a maximum of 500m CPU and 500Mi memory, and a minimum of 100m CPU and 300Mi memory.\n- The container listens on port 80.\n- Security context is configured to run the container as a non-root user with specific capabilities and a read-only root filesystem.\n- A temporary volume is mounted at '/tmp' using an in-memory empty directory.\n- The Deployment is scheduled to run on nodes with the Linux operating system.\n\nK8s manifest: sock-shop-2/manifests/12-orders-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: orders\n  annotations:\n        prometheus.io/scrape: 'true'\n  labels:\n    name: orders\n  namespace: sock-shop\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 80\n    targetPort: 80\n  selector:\n    name: orders\n\n```\nSummary of sock-shop-2/manifests/12-orders-svc.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'orders'.\n- It is annotated to enable Prometheus scraping with 'prometheus.io/scrape: true'.\n- The Service is labeled with 'name: orders'.\n- It is deployed in the 'sock-shop' namespace.\n- The Service exposes port 80 and directs traffic to the same port on the target pods.\n- It uses a selector to match pods with the label 'name: orders'.\n\nK8s manifest: sock-shop-2/manifests/13-orders-db-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: orders-db\n  labels:\n    name: orders-db\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: orders-db\n  template:\n    metadata:\n      labels:\n        name: orders-db\n    spec:\n      containers:\n      - name: orders-db\n        image: mongo\n        ports:\n        - name: mongo\n          containerPort: 27017\n        securityContext:\n          capabilities:\n            drop:\n              - all\n            add:\n              - CHOWN\n              - SETGID\n              - SETUID\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n      volumes:\n        - name: tmp-volume\n          emptyDir:\n            medium: Memory\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n\n```\nSummary of sock-shop-2/manifests/13-orders-db-dep.yaml:\n- This manifest defines a Deployment in Kubernetes.\n- The Deployment is named 'orders-db' and is located in the 'sock-shop' namespace.\n- It specifies 2 replicas of the 'orders-db' pod to be created.\n- The pods are labeled with 'name: orders-db' for identification and selection.\n- Each pod runs a single container using the 'mongo' image.\n- The container exposes port 27017, which is the default port for MongoDB.\n- Security settings are applied to drop all capabilities and add only CHOWN, SETGID, and SETUID.\n- The root filesystem of the container is set to read-only for security purposes.\n- A temporary volume is mounted at '/tmp' using an in-memory emptyDir volume.\n- The pods are scheduled to run on nodes with the operating system labeled as 'linux'.\n\nK8s manifest: sock-shop-2/manifests/14-orders-db-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: orders-db\n  labels:\n    name: orders-db\n  namespace: sock-shop\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 27017\n    targetPort: 27017\n  selector:\n    name: orders-db\n\n```\nSummary of sock-shop-2/manifests/14-orders-db-svc.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'orders-db'.\n- It is located in the 'sock-shop' namespace.\n- The Service is configured to expose port 27017.\n- It targets the same port (27017) on the pods it selects.\n- The Service uses a selector to match pods with the label 'name: orders-db'.\n\nK8s manifest: sock-shop-2/manifests/15-payment-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: payment\n  labels:\n    name: payment\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: payment\n  template:\n    metadata:\n      labels:\n        name: payment\n    spec:\n      containers:\n      - name: payment\n        image: weaveworksdemos/payment:0.4.3\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 99m\n            memory: 100Mi\n        ports:\n        - containerPort: 80\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 10001\n          capabilities:\n            drop:\n              - all\n            add:\n              - NET_BIND_SERVICE\n          readOnlyRootFilesystem: true\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 300\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 180\n          periodSeconds: 3\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n\n```\nSummary of sock-shop-2/manifests/15-payment-dep.yaml:\n- This manifest defines a Deployment in Kubernetes.\n- The Deployment is named 'payment' and is part of the 'sock-shop' namespace.\n- It specifies that there should be 2 replicas of the 'payment' application running.\n- The Deployment uses the Docker image 'weaveworksdemos/payment:0.4.3'.\n- Resource limits are set for the container, with a maximum of 200m CPU and 200Mi memory, and requests for 99m CPU and 100Mi memory.\n- The container listens on port 80.\n- Security settings ensure the container runs as a non-root user with user ID 10001, drops all capabilities except 'NET_BIND_SERVICE', and uses a read-only root filesystem.\n- Liveness and readiness probes are configured to check the '/health' endpoint on port 80, with initial delays of 300 and 180 seconds respectively, and a period of 3 seconds.\n- The Deployment is scheduled to run on nodes with the Linux operating system.\n\nK8s manifest: sock-shop-2/manifests/16-payment-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: payment\n  annotations:\n        prometheus.io/scrape: 'true'\n  labels:\n    name: payment\n  namespace: sock-shop\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 80\n    targetPort: 80\n  selector:\n    name: payment\n\n```\nSummary of sock-shop-2/manifests/16-payment-svc.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'payment'.\n- It is annotated for Prometheus scraping, which means it is set up for monitoring.\n- The Service is labeled with 'name: payment'.\n- It is deployed in the 'sock-shop' namespace.\n- The Service exposes port 80 and directs traffic to the same port on the selected pods.\n- The Service selects pods with the label 'name: payment'.\n\nK8s manifest: sock-shop-2/manifests/17-queue-master-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: queue-master\n  labels:\n    name: queue-master\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: queue-master\n  template:\n    metadata:\n      labels:\n        name: queue-master\n    spec:\n      containers:\n      - name: queue-master\n        image: weaveworksdemos/queue-master:0.3.1\n        env:\n         - name: JAVA_OPTS\n           value: -Xms64m -Xmx128m -XX:+UseG1GC -Djava.security.egd=file:/dev/urandom -Dspring.zipkin.enabled=false\n        resources:\n          limits:\n            cpu: 300m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 300Mi\n        ports:\n        - containerPort: 80\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n\n```\nSummary of sock-shop-2/manifests/17-queue-master-dep.yaml:\n- This manifest defines a Deployment in Kubernetes.\n- The Deployment is named 'queue-master' and is located in the 'sock-shop' namespace.\n- It specifies that there should be 2 replicas (instances) of the 'queue-master' application running.\n- The Deployment uses a container image 'weaveworksdemos/queue-master:0.3.1'.\n- Environment variables are set for the container, including Java options for memory management and garbage collection.\n- Resource limits and requests are defined, with a CPU limit of 300m and memory limit of 500Mi, and requests for 100m CPU and 300Mi memory.\n- The container exposes port 80 for network traffic.\n- The Deployment is configured to run on nodes with the Linux operating system.\n\nK8s manifest: sock-shop-2/manifests/18-queue-master-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: queue-master\n  annotations:\n        prometheus.io/scrape: 'true'\n  labels:\n    name: queue-master\n  namespace: sock-shop\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 80\n    targetPort: 80\n  selector:\n    name: queue-master\n```\nSummary of sock-shop-2/manifests/18-queue-master-svc.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'queue-master'.\n- It is annotated for Prometheus scraping with 'prometheus.io/scrape: true'.\n- The Service is labeled with 'name: queue-master'.\n- It is deployed in the 'sock-shop' namespace.\n- The Service exposes port 80 and directs traffic to the same port on the selected pods.\n- It selects pods with the label 'name: queue-master' to route traffic to.\n\nK8s manifest: sock-shop-2/manifests/19-rabbitmq-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rabbitmq\n  labels:\n    name: rabbitmq\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: rabbitmq\n  template:\n    metadata:\n      labels:\n        name: rabbitmq\n      annotations:\n        prometheus.io/scrape: \"false\"\n    spec:\n      containers:\n      - name: rabbitmq\n        image: rabbitmq:3.6.8-management\n        ports:\n        - containerPort: 15672\n          name: management\n        - containerPort: 5672\n          name: rabbitmq\n        securityContext:\n          capabilities:\n            drop:\n              - all\n            add:\n              - CHOWN\n              - SETGID\n              - SETUID\n              - DAC_OVERRIDE\n          readOnlyRootFilesystem: true\n      - name: rabbitmq-exporter\n        image: kbudde/rabbitmq-exporter\n        ports:\n        - containerPort: 9090\n          name: exporter\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n\n```\nSummary of sock-shop-2/manifests/19-rabbitmq-dep.yaml:\n- This manifest defines a Deployment for RabbitMQ in Kubernetes.\n- It is set to run in the 'sock-shop' namespace.\n- The Deployment is named 'rabbitmq' and is labeled accordingly.\n- It specifies 2 replicas, meaning there will be 2 instances of RabbitMQ running.\n- The Deployment uses a selector to match pods with the label 'name: rabbitmq'.\n- The pod template includes two containers: one for RabbitMQ and another for a RabbitMQ exporter.\n- The RabbitMQ container uses the image 'rabbitmq:3.6.8-management'.\n- It exposes two ports: 15672 for management and 5672 for RabbitMQ operations.\n- Security context is set to drop all capabilities and add specific ones like CHOWN, SETGID, SETUID, and DAC_OVERRIDE.\n- The root filesystem is set to read-only for security purposes.\n- The RabbitMQ exporter container uses the image 'kbudde/rabbitmq-exporter' and exposes port 9090.\n- The Deployment is configured to run on nodes with the label 'beta.kubernetes.io/os: linux'.\n- Annotations are set to prevent Prometheus from scraping metrics from this deployment.\n\nK8s manifest: sock-shop-2/manifests/20-rabbitmq-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: rabbitmq\n  annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n  labels:\n    name: rabbitmq\n  namespace: sock-shop\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 5672\n    name: rabbitmq\n    targetPort: 5672\n  - port: 9090\n    name: exporter\n    targetPort: exporter\n    protocol: TCP\n  selector:\n    name: rabbitmq\n\n```\nSummary of sock-shop-2/manifests/20-rabbitmq-svc.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'rabbitmq'.\n- It is annotated for Prometheus scraping on port 9090.\n- The Service is labeled with 'name: rabbitmq'.\n- It is deployed in the 'sock-shop' namespace.\n- The Service exposes two ports: 5672 for RabbitMQ and 9090 for an exporter.\n- The protocol used for the ports is TCP.\n- The Service selects pods with the label 'name: rabbitmq'.\n\nK8s manifest: sock-shop-2/manifests/21-session-db-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: session-db\n  labels:\n    name: session-db\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: session-db\n  template:\n    metadata:\n      labels:\n        name: session-db\n      annotations:\n        prometheus.io.scrape: \"false\"\n    spec:\n      containers:\n      - name: session-db\n        image: redis:alpine\n        ports:\n        - name: redis\n          containerPort: 6379\n        securityContext:\n          capabilities:\n            drop:\n              - all\n            add:\n              - CHOWN\n              - SETGID\n              - SETUID\n          readOnlyRootFilesystem: true\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n\n```\nSummary of sock-shop-2/manifests/21-session-db-dep.yaml:\n- This manifest defines a Deployment in Kubernetes.\n- The Deployment is named 'session-db' and is located in the 'sock-shop' namespace.\n- It specifies that there should be 2 replicas of the 'session-db' pod running.\n- The pods are selected based on the label 'name: session-db'.\n- Each pod runs a single container using the 'redis:alpine' image.\n- The container exposes port 6379, which is commonly used by Redis.\n- Security settings are applied to drop all capabilities and only add CHOWN, SETGID, and SETUID, with a read-only root filesystem for enhanced security.\n- The pods are scheduled to run on nodes with the operating system labeled as Linux.\n\nK8s manifest: sock-shop-2/manifests/22-session-db-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: session-db\n  labels:\n    name: session-db\n  namespace: sock-shop\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 6379\n    targetPort: 6379\n  selector:\n    name: session-db\n\n```\nSummary of sock-shop-2/manifests/22-session-db-svc.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'session-db'.\n- It is located in the 'sock-shop' namespace.\n- The Service is configured to expose port 6379.\n- It targets the same port (6379) on the selected pods.\n- The Service uses a selector to match pods with the label 'name: session-db'.\n\nK8s manifest: sock-shop-2/manifests/23-shipping-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: shipping\n  labels:\n    name: shipping\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: shipping\n  template:\n    metadata:\n      labels:\n        name: shipping\n    spec:\n      containers:\n      - name: shipping\n        image: weaveworksdemos/shipping:0.4.8\n        env:\n         - name: ZIPKIN\n           value: zipkin.jaeger.svc.cluster.local\n         - name: JAVA_OPTS\n           value: -Xms64m -Xmx128m -XX:+UseG1GC -Djava.security.egd=file:/dev/urandom -Dspring.zipkin.enabled=false\n        resources:\n          limits:\n            cpu: 300m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 300Mi\n        ports:\n        - containerPort: 80\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 10001\n          capabilities:\n            drop:\n              - all\n            add:\n              - NET_BIND_SERVICE\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n      volumes:\n        - name: tmp-volume\n          emptyDir:\n            medium: Memory\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n\n```\nSummary of sock-shop-2/manifests/23-shipping-dep.yaml:\n- This manifest defines a Deployment in Kubernetes.\n- The Deployment is named 'shipping' and is located in the 'sock-shop' namespace.\n- It specifies that there should be 2 replicas of the 'shipping' application running.\n- The Deployment uses the Docker image 'weaveworksdemos/shipping:0.4.8'.\n- Environment variables are set for the application, including 'ZIPKIN' and 'JAVA_OPTS'.\n- Resource limits and requests are defined, with limits set to 300m CPU and 500Mi memory, and requests set to 100m CPU and 300Mi memory.\n- The application listens on port 80.\n- Security context is configured to run the container as a non-root user with user ID 10001, and it drops all capabilities except 'NET_BIND_SERVICE'.\n- The root filesystem is set to be read-only.\n- A temporary volume is mounted at '/tmp', using an in-memory emptyDir volume.\n- The Deployment is scheduled to run on nodes with the operating system labeled as 'linux'.\n\nK8s manifest: sock-shop-2/manifests/24-shipping-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: shipping\n  annotations:\n        prometheus.io/scrape: 'true'\n  labels:\n    name: shipping\n  namespace: sock-shop\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 80\n    targetPort: 80\n  selector:\n    name: shipping\n\n\n```\nSummary of sock-shop-2/manifests/24-shipping-svc.yaml:\n- This is a Kubernetes Service manifest.\n- The service is named 'shipping'.\n- It is annotated for Prometheus scraping with 'prometheus.io/scrape: true'.\n- The service is labeled with 'name: shipping'.\n- It is deployed in the 'sock-shop' namespace.\n- The service exposes port 80 and directs traffic to the same port on the selected pods.\n- It selects pods with the label 'name: shipping' to route traffic to.\n\nK8s manifest: sock-shop-2/manifests/25-user-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user\n  labels:\n    name: user\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: user\n  template:\n    metadata:\n      labels:\n        name: user\n    spec:\n      containers:\n      - name: user\n        image: weaveworksdemos/user:0.4.7\n        resources:\n          limits:\n            cpu: 300m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 80\n        env:\n        - name: mongo\n          value: user-db:27017\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 10001\n          capabilities:\n            drop:\n              - all\n            add:\n              - NET_BIND_SERVICE\n          readOnlyRootFilesystem: true\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 300\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 180\n          periodSeconds: 3\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n\n```\nSummary of sock-shop-2/manifests/25-user-dep.yaml:\n- This manifest defines a Deployment in Kubernetes.\n- The Deployment is named 'user' and is located in the 'sock-shop' namespace.\n- It specifies that there should be 2 replicas of the 'user' application running.\n- The Deployment uses the Docker image 'weaveworksdemos/user:0.4.7'.\n- Resource limits are set for the container: 300m CPU and 200Mi memory, with requests for 100m CPU and 100Mi memory.\n- The container listens on port 80.\n- An environment variable 'mongo' is set with the value 'user-db:27017'.\n- Security context is configured to run the container as a non-root user with user ID 10001.\n- The container has a read-only root filesystem and drops all capabilities except 'NET_BIND_SERVICE'.\n- Liveness and readiness probes are configured to check the '/health' endpoint on port 80.\n- The liveness probe starts after 300 seconds and checks every 3 seconds.\n- The readiness probe starts after 180 seconds and checks every 3 seconds.\n- The Deployment is scheduled to run on nodes with the Linux operating system.\n\nK8s manifest: sock-shop-2/manifests/26-user-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: user\n  annotations:\n        prometheus.io/scrape: 'true'\n  labels:\n    name: user\n  namespace: sock-shop\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 80\n    targetPort: 80\n  selector:\n    name: user\n\n\n```\nSummary of sock-shop-2/manifests/26-user-svc.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'user'.\n- It is annotated for Prometheus scraping, which means it is set up for monitoring.\n- The Service is labeled with 'name: user'.\n- It is deployed in the 'sock-shop' namespace.\n- The Service listens on port 80 and forwards traffic to the same port on the selected pods.\n- The Service selects pods with the label 'name: user'.\n\nK8s manifest: sock-shop-2/manifests/27-user-db-dep.yaml\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-db\n  labels:\n    name: user-db\n  namespace: sock-shop\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      name: user-db\n  template:\n    metadata:\n      labels:\n        name: user-db\n    spec:\n      containers:\n      - name: user-db\n        image: weaveworksdemos/user-db:0.3.0\n\n        ports:\n        - name: mongo\n          containerPort: 27017\n        securityContext:\n          capabilities:\n            drop:\n              - all\n            add:\n              - CHOWN\n              - SETGID\n              - SETUID\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n      volumes:\n        - name: tmp-volume\n          emptyDir:\n            medium: Memory\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n\n```\nSummary of sock-shop-2/manifests/27-user-db-dep.yaml:\n- This manifest defines a Deployment in Kubernetes.\n- The Deployment is named 'user-db' and is located in the 'sock-shop' namespace.\n- It specifies that there should be 2 replicas of the 'user-db' pod running.\n- The pods are selected based on the label 'name: user-db'.\n- Each pod runs a single container using the image 'weaveworksdemos/user-db:0.3.0'.\n- The container exposes port 27017, labeled as 'mongo', which is typically used for MongoDB.\n- Security settings are applied to drop all capabilities and only add CHOWN, SETGID, and SETUID, with a read-only root filesystem.\n- A temporary volume is mounted at '/tmp' using an in-memory emptyDir volume.\n- The pods are scheduled to run on nodes with the operating system labeled as 'linux'.\n\nK8s manifest: sock-shop-2/manifests/28-user-db-svc.yaml\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: user-db\n  labels:\n    name: user-db\n  namespace: sock-shop\nspec:\n  ports:\n    # the port that this service should serve on\n  - port: 27017\n    targetPort: 27017\n  selector:\n    name: user-db\n\n\n```\nSummary of sock-shop-2/manifests/28-user-db-svc.yaml:\n- This manifest defines a Kubernetes Service.\n- The Service is named 'user-db'.\n- It is labeled with 'name: user-db'.\n- The Service is created in the 'sock-shop' namespace.\n- It exposes port 27017 and directs traffic to the same port on the target pods.\n- The Service selects pods with the label 'name: user-db' to route traffic to them.\n\nThe resiliency issues/weaknesses in the system are as follows:\nIssue #0: Missing Resource Requests\n  - details: Pods may not get scheduled if the cluster is under resource pressure, leading to potential downtime.\n  - manifests having the issues: ['sock-shop-2/manifests/03-carts-db-dep.yaml', 'sock-shop-2/manifests/07-catalogue-db-dep.yaml', 'sock-shop-2/manifests/13-orders-db-dep.yaml', 'sock-shop-2/manifests/19-rabbitmq-dep.yaml', 'sock-shop-2/manifests/21-session-db-dep.yaml', 'sock-shop-2/manifests/27-user-db-dep.yaml']\n  - problematic config: The deployments for carts-db, catalogue-db, orders-db, rabbitmq, session-db, and user-db do not specify resource requests.\n\nIssue #1: Single Replica Deployment\n  - details: The front-end deployment has only one replica, which can lead to downtime if the pod fails.\n  - manifests having the issues: ['sock-shop-2/manifests/09-front-end-dep.yaml']\n  - problematic config: spec.replicas: 1\n\nIssue #2: Missing Liveness and Readiness Probes\n  - details: Without liveness and readiness probes, Kubernetes cannot determine the health of the containers, which may lead to serving traffic to unhealthy pods.\n  - manifests having the issues: ['sock-shop-2/manifests/01-carts-dep.yaml', 'sock-shop-2/manifests/03-carts-db-dep.yaml', 'sock-shop-2/manifests/05-catalogue-dep.yaml', 'sock-shop-2/manifests/07-catalogue-db-dep.yaml', 'sock-shop-2/manifests/11-orders-dep.yaml', 'sock-shop-2/manifests/13-orders-db-dep.yaml', 'sock-shop-2/manifests/15-payment-dep.yaml', 'sock-shop-2/manifests/17-queue-master-dep.yaml', 'sock-shop-2/manifests/19-rabbitmq-dep.yaml', 'sock-shop-2/manifests/21-session-db-dep.yaml', 'sock-shop-2/manifests/23-shipping-dep.yaml', 'sock-shop-2/manifests/25-user-dep.yaml', 'sock-shop-2/manifests/27-user-db-dep.yaml']\n  - problematic config: The deployments for carts, carts-db, catalogue, catalogue-db, orders, orders-db, payment, queue-master, rabbitmq, session-db, shipping, user, and user-db do not have liveness or readiness probes configured.\n\nIssue #3: Hardcoded Environment Variables\n  - details: Hardcoded environment variables, such as passwords, can lead to security vulnerabilities.\n  - manifests having the issues: ['sock-shop-2/manifests/07-catalogue-db-dep.yaml']\n  - problematic config: env: MYSQL_ROOT_PASSWORD is hardcoded with value: fake_password.\n\nThe expected type of application on the system (i.e., K8s manfests):\nSock Shop is an e-commerce application that simulates an online sock store. It offers users the ability to browse a catalogue of socks, add items to a cart, place orders, and manage user accounts. The application is composed of multiple microservices, each responsible for a specific function, such as managing the product catalogue, handling user sessions, processing payments, and managing orders.; The manifests provided are part of a well-known microservices demo application called 'Sock Shop'. This application is designed to demonstrate the use of microservices architecture and Kubernetes orchestration. The application simulates an e-commerce website for selling socks, with various services such as carts, catalogue, orders, payment, and user management. The file names, labels, and Docker images used in the manifests (e.g., 'weaveworksdemos') are indicative of the Sock Shop application, which is a common reference application in the Kubernetes community.\n\n# Here is the hypothesis for my system:\nThe hypothesis is \"The steady states of the sytem are maintained even when the fault scenario occurs (i.e., when the faults are injected)\".\nThe steady states here are as follows:\n2 steady states are defined.\n1st steady states:\n- Name: carts-db-replicas\n- Description: The first issue to address is the 'Missing Resource Requests' for the 'carts-db' deployment. This is a critical issue because without resource requests, the pods may not get scheduled if the cluster is under resource pressure, leading to potential downtime. To ensure the system's steady state, we need to verify that the 'carts-db' deployment maintains the expected number of replicas, which is 2, even under resource constraints. This can be measured by checking the number of running pods for the 'carts-db' deployment.\n- Threshold for the steady state: At least 1 ready replica 100% of the time and 2 ready replicas at least 80% of the time during the monitoring period.; The steady state we are considering is the number of replicas for the 'carts-db' deployment. The current state shows that there are 2 total replicas and 2 ready replicas consistently over a period of 5 seconds. This indicates that the system is currently stable with all replicas being ready. Given that the deployment is configured to have 2 replicas, the threshold should ensure that both replicas are ready to maintain the steady state. To account for some fluctuations, we can set a tolerance that allows for a brief period where not all replicas are ready, but the system should recover quickly. A reasonable threshold would be to have at least 1 ready replica 100% of the time and both replicas ready at least 80% of the time during the monitoring period. This ensures that the system remains operational even if one replica temporarily becomes unavailable.\n- Whether the steady state meets the threshold is determined by the following Python script with K8s API:\n```\nimport os\nimport time\nimport argparse\nfrom kubernetes import client, config\nfrom unittest_base import K8sAPIBase\n\nclass TestCartsDBReplicas(K8sAPIBase):\n    def __init__(self, namespace='sock-shop', deployment_name='carts-db', duration=5):\n        super().__init__()\n        self.namespace = namespace\n        self.deployment_name = deployment_name\n        self.duration = duration\n        # Use the correct API client for deployments\n        self.apps_v1 = client.AppsV1Api()\n\n    def test_replicas_threshold(self):\n        # Initialize counters for ready replicas\n        total_checks = 0\n        ready_replicas_count = 0\n        fully_ready_replicas_count = 0\n\n        for _ in range(self.duration):\n            try:\n                # Read the deployment status using the correct API client\n                deployment = self.apps_v1.read_namespaced_deployment(self.deployment_name, self.namespace)\n                replicas = deployment.status.replicas\n                ready_replicas = deployment.status.ready_replicas\n                print(f\"Total replicas: {replicas}, Ready replicas: {ready_replicas}\")\n\n                # Increment the total checks\n                total_checks += 1\n\n                # Check if at least 1 replica is ready\n                if ready_replicas >= 1:\n                    ready_replicas_count += 1\n\n                # Check if both replicas are ready\n                if ready_replicas == 2:\n                    fully_ready_replicas_count += 1\n\n            except client.exceptions.ApiException as e:\n                print(f\"Exception when calling AppsV1Api->read_namespaced_deployment: {e}\")\n\n            # Wait for 1 second before the next check\n            time.sleep(1)\n\n        # Calculate the percentage of time conditions are met\n        one_ready_percentage = (ready_replicas_count / total_checks) * 100\n        two_ready_percentage = (fully_ready_replicas_count / total_checks) * 100\n\n        # Assert the threshold conditions\n        assert one_ready_percentage == 100, \"At least 1 ready replica was not available 100% of the time.\"\n        assert two_ready_percentage >= 80, \"2 ready replicas were not available at least 80% of the time.\"\n\n        print(\"Test passed: Steady state conditions are satisfied.\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='Test carts-db replicas threshold')\n    parser.add_argument('--duration', type=int, default=5, help='Duration to check the replicas')\n    args = parser.parse_args()\n\n    # Create an instance of the test class with the specified duration\n    test = TestCartsDBReplicas(duration=args.duration)\n    # Run the test\n    test.test_replicas_threshold()\n\n\nif __name__ == '__main__':\n    main()\n\n```2nd steady states:\n- Name: front-end-replica\n- Description: The next issue to address is the 'Single Replica Deployment' for the 'front-end' deployment. This is a critical issue because having only one replica can lead to downtime if the pod fails. To ensure the system's steady state, we need to verify that the 'front-end' deployment maintains at least one running pod at all times. This can be measured by checking the number of running pods for the 'front-end' deployment. Given that the deployment is configured to have only 1 replica, the threshold should ensure that this single replica is always ready to maintain the steady state. A reasonable threshold would be to have at least 1 ready replica 100% of the time during the monitoring period. This ensures that the system remains operational even if there are temporary issues with the pod.\n- Threshold for the steady state: At least 1 ready replica must be present 100% of the time during the monitoring period.; The steady state we are considering is the availability of the 'front-end' deployment, which currently has only one replica. The current state shows that the 'front-end' deployment has 1 ready replica consistently over a monitoring period of 5 seconds. To ensure the system remains operational, the threshold should be set such that the 'front-end' deployment maintains at least 1 ready replica 100% of the time during the monitoring period. This accounts for the fact that there is only one replica, and any downtime would mean the service is unavailable. Given the current state satisfies this condition, the threshold should be set to ensure that at least 1 ready replica is present 100% of the time during the monitoring period.\n- Whether the steady state meets the threshold is determined by the following Python script with K8s API:\n```\nimport os\nimport time\nimport argparse\nfrom kubernetes import client, config\nfrom unittest_base import K8sAPIBase\n\nclass TestFrontEndReplica(K8sAPIBase):\n    def __init__(self, namespace, deployment_name, duration):\n        super().__init__()\n        self.namespace = namespace\n        self.deployment_name = deployment_name\n        self.duration = duration\n        # Use AppsV1Api to interact with deployments\n        self.apps_v1 = client.AppsV1Api()\n\n    def test_steady_state(self):\n        ready_replicas_count = 0\n\n        # Loop for the specified duration\n        for _ in range(self.duration):\n            # Get the deployment status\n            resp = self.apps_v1.read_namespaced_deployment_status(self.deployment_name, self.namespace)\n            ready_replicas = resp.status.ready_replicas or 0\n            print(f\"Ready replicas for {self.deployment_name}: {ready_replicas}\")\n\n            # Check if the number of ready replicas is at least 1\n            if ready_replicas >= 1:\n                ready_replicas_count += 1\n\n            # Wait for 1 second before the next check\n            time.sleep(1)\n\n        # Calculate the percentage of time the deployment was ready\n        readiness_percentage = (ready_replicas_count / self.duration) * 100\n        print(f\"{self.deployment_name} was ready {ready_replicas_count}/{self.duration} times.\")\n\n        # Assert that the deployment was ready 100% of the time\n        assert readiness_percentage == 100, f\"{self.deployment_name} readiness was {readiness_percentage}%, expected 100%.\"\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='Test front-end replica readiness')\n    parser.add_argument('--duration', type=int, default=5, help='Duration to check the replicas')\n    args = parser.parse_args()\n\n    # Create a test instance and run the test\n    test = TestFrontEndReplica('sock-shop', 'front-end', args.duration)\n    test.test_steady_state()\n\n\nif __name__ == '__main__':\n    main()\n```\n\nThe fault scenario here is as follows:\n\nAn assumed fault scenario is as follows:\n- Event: Black Friday Sale\n- Used Chaos Engineering tool: Chaos Mesh\n- Faults to simulate the event: [[Fault(name='StressChaos', name_id=0, params={'mode': 'all', 'selector': {'namespaces': ['sock-shop'], 'labelSelectors': {'name': 'carts-db'}}, 'stressors': {'cpu': {'workers': 2, 'load': 80}}, 'containerNames': ['carts-db']})], [Fault(name='PodChaos', name_id=0, params={'action': 'pod-kill', 'mode': 'one', 'selector': {'namespaces': ['sock-shop'], 'labelSelectors': {'name': 'front-end'}}, 'value': '1'})]]\n- Description: During a Black Friday sale, the system is expected to handle a significant increase in traffic, which can stress the system's resources and reveal weaknesses. The 'carts-db' deployment lacks resource requests, which can lead to scheduling issues under resource pressure. The 'front-end' deployment has only one replica, making it a single point of failure. To simulate the Black Friday event, we will first inject a 'StressChaos' fault to simulate high CPU usage on the 'carts-db' pods, testing if the system can maintain the steady state of having at least 1 ready replica 100% of the time and 2 ready replicas 80% of the time. Next, we will inject a 'PodChaos' fault to kill the 'front-end' pod, testing if the system can maintain the steady state of having at least 1 ready replica 100% of the time. This sequence simulates the increased load and potential failures during a high-traffic event like Black Friday.\n\n# Here is the overview of my Chaos-Engineering experiment to verify the hypothesis:\nThe chaos engineering experiment is structured into three phases, each lasting 20 seconds, to test the system's resilience. In the pre-validation phase, both unit tests for verifying steady states are executed simultaneously for the entire 20 seconds. The 'carts-db-replicas' test checks that at least one replica is ready 100% of the time and two replicas are ready 80% of the time, while the 'front-end-replica' test ensures one replica is ready 100% of the time. In the fault-injection phase, the 'StressChaos' fault is injected on the 'carts-db' deployment to simulate high CPU usage, starting immediately and lasting for 10 seconds. Concurrently, the 'carts-db-replicas' unit test runs for 10 seconds to verify readiness under stress. At the 10-second mark, the 'PodChaos' fault is injected to kill the 'front-end' pod, lasting for 5 seconds, while the 'front-end-replica' unit test runs for 5 seconds to check readiness during the fault. In the post-validation phase, the 'carts-db-replicas' unit test runs for the first 10 seconds to verify recovery, followed by the 'front-end-replica' unit test for the next 10 seconds, ensuring both deployments return to their steady states.\n\n# The experiment's results are as follows:\nPassed unittests:\n- pre-unittest-carts-db-replicas\n- pre-unittest-front-end-replica\n- fault-unittest-carts-db-replicas\n- post-unittest-carts-db-replicas\nFailed unittests:\n- fault-unittest-front-end-replica\n```log\nTraceback (most recent call last):\n  File \"/chaos-eater/sandbox/cycle_20241127_043136/hypothesis/unittest_front-end-replica_mod1.py\", line 52, in <module>\n    main()\n  File \"/chaos-eater/sandbox/cycle_20241127_043136/hypothesis/unittest_front-end-replica_mod1.py\", line 48, in main\n    test.test_steady_state()\n  File \"/chaos-eater/sandbox/cycle_20241127_043136/hypothesis/unittest_front-end-replica_mod1.py\", line 38, in test_steady_state\n    assert readiness_percentage == 100, f\"{self.deployment_name} readiness was {readiness_percentage}%, expected 100%.\"\nAssertionError: front-end readiness was 0.0%, expected 100%.\nReady replicas for front-end: 0\nReady replicas for front-end: 0\nReady replicas for front-end: 0\nReady replicas for front-end: 0\nReady replicas for front-end: 0\nfront-end was ready 0/5 times.\n\n```\n\n- post-unittest-front-end-replica\n```log\nReady replicas for front-end: 0\nReady replicas for front-end: 0\nReady replicas for front-end: 0\nReady replicas for front-end: 0\nReady replicas for front-end: 0\nTraceback (most recent call last):\n  File \"/chaos-eater/sandbox/cycle_20241127_043136/hypothesis/unittest_front-end-replica_mod1.py\", line 52, in <module>\nReady replicas for front-end: 0\nReady replicas for front-end: 0\nReady replicas for front-end: 0\nReady replicas for front-end: 0\nReady replicas for front-end: 0\nfront-end was ready 0/10 times.\n    main()\n  File \"/chaos-eater/sandbox/cycle_20241127_043136/hypothesis/unittest_front-end-replica_mod1.py\", line 48, in main\n    test.test_steady_state()\n  File \"/chaos-eater/sandbox/cycle_20241127_043136/hypothesis/unittest_front-end-replica_mod1.py\", line 38, in test_steady_state\n    assert readiness_percentage == 100, f\"{self.deployment_name} readiness was {readiness_percentage}%, expected 100%.\"\nAssertionError: front-end readiness was 0.0%, expected 100%.\n\n```\n\n\nNow, please analyze the results and provide an analysis report rich in insights.\nAI: ```json\n{\"report\":"
            ],
            "{\"report\": \"The chaos engineering experiment aimed to test the resilience of the 'sock-shop' system under simulated fault conditions, specifically focusing on the 'carts-db' and 'front-end' deployments. The experiment was structured into three phases: pre-validation, fault-injection, and post-validation. The results indicate that while the 'carts-db' deployment maintained its steady state throughout the experiment, the 'front-end' deployment failed to meet the expected steady state during both the fault-injection and post-validation phases.\\n\\n1. **Pre-Validation Phase:**\\n   - Both the 'carts-db-replicas' and 'front-end-replica' tests passed, indicating that the system was in a stable state before any faults were injected. This confirms that the initial configuration of the system was functioning as expected.\\n\\n2. **Fault-Injected Phase:**\\n   - The 'carts-db-replicas' test passed during the fault-injection phase, demonstrating that the 'carts-db' deployment was resilient to the 'StressChaos' fault, which simulated high CPU usage. This suggests that despite the lack of resource requests, the 'carts-db' deployment was able to maintain at least one ready replica 100% of the time and two ready replicas 80% of the time.\\n   - The 'front-end-replica' test failed during the fault-injection phase. The 'PodChaos' fault, which involved killing the 'front-end' pod, resulted in 0 ready replicas throughout the test duration. This failure highlights the critical issue of having only a single replica for the 'front-end' deployment, making it a single point of failure. The system was unable to recover or maintain availability when the single pod was terminated.\\n\\n3. **Post-Validation Phase:**\\n   - The 'carts-db-replicas' test continued to pass in the post-validation phase, indicating that the 'carts-db' deployment returned to its steady state after the fault was removed.\\n   - The 'front-end-replica' test failed again in the post-validation phase, with 0 ready replicas recorded throughout the test. This suggests that the 'front-end' deployment did not recover after the fault was removed, likely due to the absence of additional replicas to take over when the single pod was killed.\\n\\n**Insights and Recommendations:**\\n- The failure of the 'front-end' deployment to maintain its steady state during and after the fault-injection phase underscores the importance of addressing the 'Single Replica Deployment' issue. To improve resilience, it is recommended to increase the number of replicas for the 'front-end' deployment. This will ensure that the system can maintain availability even if one pod fails.\\n- Consider implementing horizontal pod autoscaling for the 'front-end' deployment to dynamically adjust the number of replicas based on traffic and resource usage. This can help the system handle sudden spikes in demand, such as those experienced during events like Black Friday.\\n- Review and potentially adjust the readiness and liveness probes for the 'front-end' deployment to ensure that Kubernetes can accurately determine the health of the pods and take corrective actions if necessary.\\n\\nBy addressing these issues, the system's resilience can be significantly improved, reducing the risk of downtime during high-traffic events and fault scenarios.\"}\n"
        ]
    }
]